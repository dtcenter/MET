////////////////////////////////////////////////////////////////////////////////
//
// Default stat_analysis configuration file
//
////////////////////////////////////////////////////////////////////////////////

//
// The parameters listed below are used to filter the STAT data down to the
// desired subset of STAT lines over which statistics are to be computed.  Only
// those STAT lines which meet ALL of the criteria specified will be retained.
//
// The parameters set at the top level before the job list will be applied to
// each of the analysis jobs in the job list.  If no selection is listed for
// a parameter, that parameter will not be used for filtering.  If multiple
// selections are listed for a parameter, the analyses will be performed on
// their union.
//
// Additional filtering parameters may be selected for individual jobs to
// further refine the data using the appropriate job command line options.
//

//
// Stratify by MODEL column:
// Specify a comma-separated list of model names to be used for all analyses
// performed.  These selections may be further refined using the "-model"
// job command option.
// e.g. model[] = [ "model1", "model2", "model3" ];
//
model[] = [];

//
// Stratify by FCST_LEAD and OBS_LEAD columns:
// Specify a comma-separated list of forecast lead times in HH[MMSS] format to
// be used for all analyses performed.  These selections may be further refined
// using the "-fcst_lead" and "-obs_lead" job command options.
// e.g. fcst_lead[] = [ "06", "12", "180000", "240000" ];
//
fcst_lead[] = [];
obs_lead[]  = [];

//
// Stratify by FCST_VALID_BEG, FCST_VALID_END, OBS_VALID_BEG, and OBS_VALID_END
// columns:
// Specify beginning and ending valid times in YYYYMMDD[_HH[MMSS]] format to be
// used for all analyses performed.  These selections may be further refined
// using the "-fcst_valid_beg", "-fcst_valid_end", "-obs_valid_beg", and
// "-obs_valid_end" job command options.
// e.g. fcst_valid_beg = "20070102";
//      fcst_valid_end = "20070103_000000";
//
fcst_valid_beg = "";
fcst_valid_end = "";
obs_valid_beg  = "";
obs_valid_end  = "";

//
// Stratify by forecast and observation initialization times:
// Specify beginning and ending model initialization times in
// YYYYMMDD[_HH[MMSS]] format to be used for all analyses performed.  These
// selections may be further refined using the "-fcst_init_beg",
// "-fcst_init_end", "-obs_init_beg", and "-obs_init_end" job command options.
// e.g. fcst_init_beg = "20070101_00";
//      fcst_init_end = "20070102";
//
fcst_init_beg = "";
fcst_init_end = "";
obs_init_beg  = "";
obs_init_end  = "";

//
// Stratify by the forecast and observation initialization hours:
// Specify a comma-separated list of intialization hours (i.e. cycle time)
// in HH[MMSS] format to be used for all analyses performed.  This selection
// may be further refined using the "-fcst_init_hour" and "-obs_init_hour"
// job command options.
// e.g. fcst_init_hour[] = [ "00", "12" ];
//
fcst_init_hour[] = [];
obs_init_hour[]  = [];

//
// Stratify by FCST_VAR and OBS_VAR columns:
// Specify a comma-separated list of model variables to be used for all analyses
// performed.  These selections may be further refined using the "-fcst_var"
// and "-obs_var" job command options.
// e.g. fcst_var[] = [ "TMP", "APCP/24", "RH" ];
//
fcst_var[] = [];
obs_var[]  = [];

//
// Stratify by FCST_LEV and OBS_LEV columns:
// Specify a comma-separated list of model levels to be used for all analyses
// performed.  These selections may be further refined using the "-fcst_lev"
// and "-obs_lev" job command options.
// e.g. fcst_lev[] = [ "SFC", "L10", "P750-950" ];
//
fcst_lev[] = [];
obs_lev[]  = [];

//
// Stratify by the OBTYPE column:
// Specify a comma-separated list of observation types to be used for all
// analyses performed.  These selections may be further refined using the
// "-obtype" job command option.
// e.g. obtype[] = [ "ANALYS", "ADPUPA", "ADPSFC" ];
//
obtype[] = [];

//
// Stratify by the VX_MASK column:
// Specify a comma-separated list of verification masking regions to be used
// for all analyses performed.  These selections may be further refined using
// the "-vx_mask" job command option.
// e.g. vx_mask[] = [ "G212", "LMV" ];
//
vx_mask[] = [];

//
// Stratify by the INTERP_MTHD column:
// Specify a comma-separated list of interpolation methods to be used for all
// analyses performed.  These selections may be further refined using the
// "-interp_mthd" job command option.
// e.g. interp_mthd[] = [ "MIN", "MAX", "MEDIAN", "UW_MEAN", "DW_MEAN" ];
//
interp_mthd[] = [];

//
// Stratify by the INTERP_PNTS column:
// Specify a comma-separated list of interpolation points to be used for all
// analyses performed.  These selections may be further refined using the
// "-interp_pnts" job command option.
// e.g. interp_pnts[] = [ 1, 4, 9, 16 ];
//
interp_pnts[] = [];

//
// Stratify by FCST_THRESH, OBS_THRESH, and COV_THRESH columns:
// Specify a comma-separated list of thresholds to be used for all analyses
// performed.  These selections may be further refined using the "-fcst_thresh",
// "-obs_thresh", and "-cov_thresh" job command options.
// e.g. fcst_thrseh[] = [ ">0.000", "<=10.000" ];
//
fcst_thresh[] = [];
obs_thresh[]  = [];
cov_thresh[]  = [];

//
// Stratify by the ALPHA column:
// Specify a comma-separated list of alpha values to be used for all analyses
// performed.  These selections may be further refined using the "-alpha" job
// command option.
// e.g. alpha[] = [ 0.05 ];
//
alpha[] = [];

//
// Stratify by LINE_TYPE column:
// Specify a comma-separated list of line types to be used for all analyses
// performed.  These selections may be further refined using the "-line_type"
// job command option.
// e.g. line_type[] = [ "FHO" ];
//
line_type[] = [];

//
// Specify the column(s) to be used in the summary or Skill Score Index jobs.
// e.g. column[] = [ "RMSE" ];
//
column[] = [];

//
// Specify the weights to be used in the Skill Score Index job.
// e.g. weight[] = [ 4.0, 3.0, 2.0, 1.0 ];
//
weight[] = [];

////////////////////////////////////////////////////////////////////////////////
//
// Analysis job list.
//
////////////////////////////////////////////////////////////////////////////////

//
// Specify the analysis jobs to be performed.  Each entry in the joblist
// contains the specifications for a single analysis job to be performed.
// The format for an analysis job is as follows:
//
//    -job job_name
//    OPTIONAL ARGS
//
//    Where "job_name" is set to one of the following:
//
//       "filter"
//          To filter out the STAT lines matching the criteria specified above
//          and using the optional arguments below.  The output STAT lines are
//          written to the file specified using the "-dump_row" option.
//          Required Args: -dump_row
//
//       "summary"
//          To compute the mean, standard deviation, and percentiles
//          (0th, 10th, 25th, 50th, 75th, 90th, and 100th) for the statistic
//          specified using the "-line_type" and "-column" arguments.
//          Required Args: -line_type, -column
//
//       "aggregate"
//          To aggregate the STAT data for the STAT line type specified using
//          the "-line_type" argument.  The output of the job will be in the
//          same format as the input line type specified.  The following line
//          types may be aggregated:
//          -line_type FHO, CTC, MCTC,
//                     SL1L2, SAL1L2, VL1L2, VAL1L2,
//                     PCT, NBRCTC, ISC, RHIST
//          Required Args: -line_type
//
//       "aggregate_stat"
//          To aggregate the STAT data for the STAT line type specified using
//          the "-line_type" argument.  The output of the job will be the line
//          type specified using the "-out_line_type" argument.  The valid
//          combinations of "-line_type" and "-out_line_type" are listed below.
//          -line_type FHO,   CTC,    -out_line_type CTS
//          -line_type MCTC           -out_line_type MCTS
//          -line_type SL1L2, SAL1L2, -out_line_type CNT
//          -line_type VL1L2, VAL1L2, -out_line_type WDIR (wind direction)
//          -line_type PCT,           -out_line_type PSTD, PJC, PRC
//          -line_type NBRCTC,        -out_line_type NBRCTS
//          -line_type MPR,           -out_line_type FHO, CTC, CTS,
//                                                   MCTC, MCTS, CNT,
//                                                   SL1L2, SAL1L2,
//                                                   PCT, PSTD, PJC, PRC
//          Required Args:
//             -line_type, -out_line_type
//          Additional Required Args for -line_type MPR:
//             -out_fcst_thresh, -out_obs_thresh
//             When -out_line_type FHO, CTC, CTS, MCTC, MCTS,
//                                 PCT, PSTD, PJC, PRC
//          Additional Optional Args for -line_type MPR:
//             -mask_grid, -mask_poly
//
//       "ss_index"
//          The skill score index job can be configured to compute a weighted
//          average of skill scores derived from a configurable set of
//          variables, levels, lead times, and statistics.  The skill score
//          index is computed using two models, a forecast model and a
//          reference model.  For each statistic in the index, a skill score
//          is computed as:
//             SS = 1 - (S[model]*S[model])/(S[reference]*S[reference])
//          Where S is the statistic.
//          Next, a weighted average is computed over all the skill scores.
//          Lastly, an index value is computed as:
//             Index = sqrt(1/(1-SS[avg]))
//          Where SS[avg] is the weighted average of skill scores.
//          Required Args:
//             Exactly 2 entries for -model, the forecast model and reference
//             For each term of the index:
//             -fcst_var, -fcst_lev, -fcst_lead, -line_type, -column, -weight
//             Where -line_type is CNT or CTS and -column is the statistic.
//             Optionally, specify other filters for each term, -fcst_thresh.
//
//       "go_index"
//          The GO Index is a special case of the Skill Score Index consisting
//          of a predefined set of variables, levels, lead times, statistics,
//          and weights.
//          For lead times of 12, 24, 36, and 48 hours, it contains RMSE for:
//          - Wind Speed at the surface(b), 850(a), 400(a), 250(a) mb
//          - Dewpoint Temperature at the surface(b), 850(b), 700(b), 400(b) mB
//          - Temperature at the surface(b), 400(a) mB
//          - Height at 400(a) mB
//          - Sea Level Pressure(b)
//          Where (a) means weights of 4, 3, 2, 1 for the lead times, and
//          (b) means weights of 8, 6, 4, 2 for the lead times.
//          Required Args: None
//
//    Job command FILTERING options to further refine the STAT data:
//       Each optional argument may be used in the job specification multiple
//       times unless otherwise indicated. When multiple optional arguments of
//       the same type are indicated, the analysis will be performed over their
//       union:
//
//       "-model          name"
//       "-fcst_lead      HHMMSS"
//       "-obs_lead       HHMMSS"
//       "-fcst_valid_beg YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_valid_end YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_valid_beg  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_valid_end  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_beg  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_end  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_init_beg   YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_init_end   YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_hour HH[MMSS]"
//       "-obs_init_hour  HH[MMSS]"
//       "-fcst_var       name"
//       "-obs_var        name"
//       "-fcst_lev       name"
//       "-obs_lev        name"
//       "-obtype         name"
//       "-vx_mask        name"
//       "-interp_mthd    name"
//       "-interp_pnts    n"
//       "-fcst_thresh    t"
//       "-obs_thresh     t"
//       "-cov_thresh     t"
//       "-alpha          a"
//       "-line_type      type"
//       "-column         name"
//       "-weight         value"
//
//   Job command FILTERING options used when the -line_type has been set to a
//   single value.  These options take two arguments, the name of the data
//   column to be used and the min, max, or exact value for that column.
//   If multiple column eq/min/max/str options are listed, the job will be
//   performed on their intersection:
//
//       "-column_min col_name value"
//       "-column_max col_name value"
//       "-column_eq  col_name value"
//       "-column_str col_name string" separate multiple filtering strings
//                                     with commas
//
//   Job command options to DEFINE the analysis job.  Unless otherwise noted,
//   these options may only be used ONCE per analysis job:
//
//       "-dump_row        path"
//
//       "-mask_grid       name"
//       "-mask_poly       file"
//
//       "-out_line_type   name"
//       "-out_fcst_thresh value" multiple for multi-category contingency tables
//                                and probabilistic forecasts
//       "-out_obs_thresh  value" multiple for multi-category contingency tables
//       "-out_alpha       value"
//
//       "-boot_interval   value"
//       "-boot_rep_prop   value"
//       "-n_boot_rep      value"
//       "-boot_rng        value"
//       "-boot_seed       value"
//
//       "-rank_corr_flag  value"
//       "-vif_flag        value"
//
jobs[] = [
   "-job filter -dump_row ./filter_job.stat"
];

//
// Specify the alpha value to be used when computing confidence intervals.
//
out_alpha = 0.05;

//
// Specify the method to be used for computing bootstrap confidence intervals.
// The value for this is interpreted as follows:
//    (0) Use the BCa interval method (computationally intensive)
//    (1) Use the percentile interval method
//
boot_interval = 1;

//
// Specify a proportion between 0 and 1 to define the replicate sample size
// to be used when computing percentile intervals.  The replicate sample
// size is set to boot_rep_prop * n, where n is the number of raw data points.
//
// e.g boot_rep_prop = 0.80;
//
boot_rep_prop = 1.0;

//
// Specify the number of times each set of matched pair data should be
// resampled when computing bootstrap confidence intervals.  A value of
// zero disables the computation of bootstrap condifence intervals.
//
// e.g. n_boot_rep = 1000;
//
n_boot_rep = 1000;

//
// Specify the name of the random number generator to be used.  See the MET
// Users Guide for a list of possible random number generators.
//
boot_rng = "mt19937";

//
// Specify the seed value to be used when computing bootstrap confidence
// intervals.  If left unspecified, the seed will change for each run and
// the computed bootstrap confidence intervals will not be reproducable.
//
boot_seed = "";

//
// Flag to indicate whether Kendall's Tau and Spearman's Rank Correlation
// Coefficients should be computed.  Computing them over large datasets is
// computationally intensive and slows down the runtime execution significantly.
//    (0) Do not compute these correlation coefficients
//    (1) Compute these correlation coefficients
//
rank_corr_flag = 1;

//
// Flag to indicate whether a variance inflation factor should be computed when
// aggregating a time series of contingency table counts or partial sums.  The
// VIF is used to adjust the normal confidence intervals computed for the
// aggregated statistics.
//    (0) Do not compute VIF
//    (1) Compute a VIF for time series
//
vif_flag = 0;

//
// Directory where temp files should be written by the stat_analysis tool
//
tmp_dir = "/tmp";

//
// Indicate a version number for the contents of this configuration file.
// The value should generally not be modified.
//
version = "V3.0";
