////////////////////////////////////////////////////////////////////////////////
//
// Configuration file overview.
//
////////////////////////////////////////////////////////////////////////////////

The configuration files that control many of the MET tools contain formatted
ASCII text.  This format has been updated for METv4.0 and continues to be used
in subsequent releases.

Settings common to multiple tools are described in the top part of this README
file and settings specific to individual tools are described beneath the common
settings.  Please refer to the MET User's Guide in the "doc" directory for more
details about the settings if necessary.

A configuration file entry is an entry name, followed by an equal sign (=),
followed by an entry value, and is terminated by a semicolon (;).  The
configuration file itself is one large dictionary consisting of entries, some of
which are dictionaries themselves.

The configuration file language supports the following data types:
   - Dictionary:
      - Grouping of one or more entries enclosed by curly braces {}.
   - Array:
      - List of one or more entries enclosed by square braces [].
      - Array elements are separated by commas.
   - String:
      - A character string enclosed by double quotation marks "".
   - Integer:
      - A numeric integer value.
   - Float:
      - A numeric float value.
   - Boolean:
      - A boolean value (TRUE or FALSE).
   - Threshold:
      - A threshold type (<, <=, ==, !-, >=, or >) followed by a numeric value.
      - The threshold type may also be specified using two letter abbreviations
        (lt, le, eq, ne, ge, gt).
      - Multiple thresholds may be combined by specifying the logic type of AND
        (&&) or OR (||). For example, ">=5&&<=10" defines the numbers between 5
        and 10 and "==1||==2" defines numbers exactly equal to 1 or 2.
   - Piecewise-Linear Function (currently used only by MODE):
      - A list of (x, y) points enclosed in parenthesis ().
      - The (x, y) points are *NOT* separated by commas.

The context of a configuration entry matters.  If an entry cannot be found in
the expected dictionary, the MET tools recursively search for that entry in the
parent dictionaries, all the way up to the top-level configuration file
dictionary.  If you'd like to apply the same setting across all cases, you can
simply specify it once at the top-level.  Alternatively, you can specify a
setting at the appropriate dictionary level to have finer control over the
behavior.

In order to make the configuration files more readable, several descriptive
integer types have been defined in the ConfigConstants file.  These integer
names may be used on the right-hand side for many configuration file entries.

Each of the configurable MET tools expects a certain set of configuration
entries.  Examples of the MET configuration files can be found in data/config
and scripts/config.

When you pass a configuration file to a MET tool, the tool actually parses up
to four different configuration files in the following order:
   (1) Reads share/met/config/ConfigConstants to define constants.
   (2) If the tool produces PostScript output, it reads
       share/met/config/ConfigMapData to define the map data to be plotted.
   (3) Reads the default configuration file for the tool from share/met/config.
   (4) Reads the user-specified configuration file from the command line.

Many of the entries from step (3) are overwritten by the user-specified entries
from step (4).  Therefore, the configuration file you pass in on the command
line really only needs to contain entries that differ from the defaults.

Any of the configuration entries may be overwritten by the user-specified
configuration file.  For example, the map data to be plotted may be included in
the user-specified configuration file and override the default settings defined
in the share/met/config/ConfigMapData file.

The configuration file language supports the use of environment variables. They
are specified as ${ENV_VAR}, where ENV_VAR is the name of the environment
variable.  When scripting up many calls to the MET tools, you may find it
convenient to use them.  For example, when applying the same configuration to
the output from multiple models, consider defining the model name as an
environment variable which the controlling script sets prior to verifying the
output of each model.  Setting MODEL to that environment variable enables you
to use one configuration file rather than maintianing many very similar ones.

The MET_BASE variable is defined in the code at compilation time as the path
to the MET shared data.  These are things like the default configuration files,
common polygons and color scales.  MET_BASE may be used in the MET configuration
files when specifying paths and the appropriate path will be substituted in.
If MET_BASE is defined as an environment variable, its value will be used
instead of the one defined at compilation time.

The MET_GRIB_TABLES environment variable can be set to specify the location of
custom GRIB tables.  It can either be set to a specific file name or to a
directory containing custom GRIB tables files.  These file names must begin with
a "grib1" or "grib2" prefix and end with a ".txt" suffix.  Their format must
match the format used by the default GRIB table files located in
share/met/table_files.  The custom GRIB tables are read prior to the default
tables and their settings take precedence.

An error in the syntax of a configuration file will result in an error from the
MET tool stating the location of the parsing error.

////////////////////////////////////////////////////////////////////////////////
//
// Configuration settings used by the MET tools.
//
////////////////////////////////////////////////////////////////////////////////

////////////////////////////////////////////////////////////////////////////////
//
// Settings common to multiple tools
//
////////////////////////////////////////////////////////////////////////////////

//
// The "exit_on_warning" entry in ConfigConstants may be set to true or false.
// If set to true and a MET tool encounters a warning, it will immediately exit
// with bad status after writing the warning message.
//
exit_on_warning = FALSE;

//
// The "nc_compression" entry in ConfigConstants defines the compression level
// for the NetCDF variables. Setting this option in the config file of one of
// the tools overrides the default value set in ConfigConstants. The
// environment variable MET_NC_COMPRESS overrides the compression level
// from configuration file. The command line argument "-compress n" for some
// tools overrides it.
// The range is 0 to 9.
//   - 0 is to disable the compression.
//   - 1 to 9: Lower number is faster, higher number is better.
nc_compression = 0;

//
// The "output_precision" entry in ConfigConstants defines the precision
// (number of significant decimal places) to be written to the ASCII output
// files.  Setting this option in the config file of one of the tools will
// override the default value set in ConfigConstants.
//
output_precision = 5;

// The "tmp_dir" entry in ConfigConstants defines the directory for the
// temporary files. The directory must exist and be writable. The environment
// variable MET_TMP_DIR overrides the default value at the configuration file.
// Some tools override the temporary directory by the command line argument
// "-tmp_dir <diretory_name>".
tmp_dir = "/tmp";

//
// The "model" entry specifies a name for the model being verified.  This name
// is written to the MODEL column of the ASCII output generated.  If you're
// verifying multiple models, you should choose descriptive model names (no
// whitespace) to distinguish between their output.
// e.g. model = "GFS";
//
model = "WRF";

//
// The "desc" entry specifies a user-specified description for each verification
// task.  This string is written to the DESC column of the ASCII output
// generated.  It may be set separately in each "obs.field" verification task
// entry or simply once at the top level of the configuration file.  If you're
// verifying the same field multiple times with different quality control
// flags, you should choose description strings (no whitespace) to distinguish
// between their output.
// e.g. desc = "QC_9";
//
desc = "NA";

//
// The "obtype" entry specifies a name to describe the type of verifying gridded
// observation used.  This name is written to the OBTYPE column in the ASCII
// output generated.  If you're using multiple types of verifying observations,
// you should choose a descriptive name (no whitespace) to distinguish between
// their output.  When verifying against point observations the point
// observation message type value is written to the OBTYPE column.  Otherwise,
// the configuration file obtype value is written.
//
obtype = "ANALYS";

//
// The "regrid" entry is a dictionary containing information about how to handle
// input gridded data files.  The "regrid" entry specifies regridding logic
// using the following entries:
//
//   - The "to_grid" entry may be set to NONE, FCST, OBS, a named grid, the path
//     to a gridded data file defining the grid, or an explicit grid specification
//     string.
//      - to_grid = NONE;   To disable regridding.
//      - to_grid = FCST;   To regrid observations to the forecast grid.
//      - to_grid = OBS;    To regrid forecasts to the observation grid.
//      - to_grid = "G218"; To regrid both to a named grid.
//      - to_grid = "path"; To regrid both to a grid defined by a file.
//      - to_grid = "spec"; To define a grid specified as follows:
//         - lambert Nx Ny lat_ll lon_ll lon_orient D_km R_km standard_parallel_1
//           [standard_parallel_2]
//         - stereo Nx Ny lat_ll lon_ll lon_orient D_km R_km lat_scale N|S
//         - latlon Nx Ny lat_ll lon_ll delta_lat delta_lon
//         - mercator Nx Ny lat_ll lon_ll lat_ur lon_ur
//
//    - The "vld_thresh" entry specifies a proportion between 0 and 1 to define
//      the required ratio of valid data points.  When regridding, compute
//      a ratio of the number of valid data points to the total number of
//      points in the neighborhood.  If that ratio is less than this threshold,
//      write bad data for the current point.
//
//   - The "method" entry defines the regridding method to be used.
//      - Valid regridding methods:
//         - MIN         for the minimum value
//         - MAX         for the maximum value
//         - MEDIAN      for the median value
//         - UW_MEAN     for the unweighted average value
//         - DW_MEAN     for the distance-weighted average value (weight = distance^-2)
//         - LS_FIT      for a least-squares fit
//         - BILIN       for bilinear interpolation (width = 2)
//         - NEAREST     for the nearest grid point (width = 1)
//         - BUDGET      for the mass-conserving budget interpolation
//         - FORCE       to compare gridded data directly with no interpolation
//                       as long as the grid x and y dimensions match.
//         - UPPER_LEFT  for the upper left grid point (width = 1)
//         - UPPER_RIGHT for the upper right grid point (width = 1)
//         - LOWER_RIGHT for the lower right grid point (width = 1)
//         - LOWER_LEFT  for the lower left grid point (width = 1)
//
//         The BEST interpolation option is not valid for regridding.
//
//   - The "width" entry specifies a regridding width, when applicable.
//      - width = 4;        To regrid using a 4x4 box.
//
//    - The "shape" entry defines the shape of the neighborhood.
//      Valid values are "SQUARE" or "CIRCLE"
//

//
regrid = {
   to_grid    = NONE;
   method     = NEAREST;
   width      = 1;
   vld_thresh = 0.5;
   shape      = SQUARE;
}

//
// The "fcst" entry is a dictionary containing information about the field(s)
// to be verified.  This dictionary may include the following entries:
//
//   - The "field" entry is an array of dictionaries, each specifying a
//     verification task.  Each of these dictionaries may include:
//
//      - The "name" entry specifies a name for the field.
//
//      - The "level" entry specifies level information for the field.
//
//      - Setting "name" and "level" is file-format specific.  See below.
//
//      - The "prob" entry in the forecast dictionary defines probability
//        information.  It may either be set as a boolean (i.e. TRUE or FALSE)
//        or as a dictionary defining probabilistic field information.
//
//        When set as a boolean to TRUE, it indicates that the "fcst.field" data
//        should be treated as probabilities.  For example, when verifying the
//        probabilistic NetCDF output of Ensemble-Stat, one could configure the
//        Grid-Stat or Point-Stat tools as follows:
//
//           fcst = {
//              field = [ { name  = "APCP_24_A24_ENS_FREQ_gt0.0";
//                          level = "(*,*)";
//                          prob  = TRUE; } ];
//           }
//
//        Setting "prob = TRUE" indicates that the "APCP_24_A24_ENS_FREQ_gt0.0"
//        data should be processed as probabilities.
//
//        When set as a dictionary, it defines the probabilistic field to be
//        used.  For example, when verifying GRIB files containing probabilistic
//        data,  one could configure the Grid-Stat or Point-Stat tools as
//        follows:
//
//           fcst = {
//              field = [ { name = "PROB"; level = "A24";
//                          prob = { name = "APCP"; thresh_lo = 2.54; } },
//                        { name = "PROB"; level = "P850";
//                          prob = { name = "TMP"; thresh_hi = 273; } } ];
//           }
//
//        The example above selects two probabilistic fields.  In both, "name"
//        is set to "PROB", the GRIB abbreviation for probabilities. The "level"
//        entry defines the level information (i.e. "A24" for a 24-hour
//        accumulation and "P850" for 850mb).  The "prob" dictionary defines the
//        event for which the probability is defined.  The "thresh_lo"
//        (i.e. APCP > 2.54) and/or "thresh_hi" (i.e. TMP < 273) entries are
//        used to define the event threshold(s).
//
//        Probability fields should contain values in the range
//        [0, 1] or [0, 100]. However, when MET encounters a probability field
//        with a range [0, 100], it will automatically rescale it to be [0, 1]
//        before applying the probabilistic verification methods.
//
//      - Set "prob_as_scalar = TRUE" to override the processing of probability
//        data.  When the "prob" entry is set as a dictionary to define the field
//        of interest, setting "prob_as_scalar = TRUE" indicates that this data
//        should be processed as regular scalars rather than probabilities.
//        For example, this option can be used to compute traditional 2x2
//        contingency tables and neighborhood verification statistics for
//        probability data.  It can also be used to compare two probability
//        fields directly.  When this flag is set, probability values are
//        automatically rescaled from the range [0, 100] to [0, 1].
//
//      - The "cat_thresh" entry is an array of thresholds to be used when
//        computing categorical statistics.
//
//      - The "cnt_thresh" entry is an array of thresholds for filtering
//        data prior to computing continuous statistics and partial sums.
//
//      - The "cnt_logic" entry may be set to UNION, INTERSECTION, or SYMDIFF
//        and controls the logic for how the forecast and observed cnt_thresh
//        settings are combined when filtering matched pairs of forecast and
//        observed values.
//
//   - The "file_type" entry specifies the input file type rather than letting
//     the code determine it itself.  For valid file_type values, see "File types"
//     in the data/config/ConfigConstants file.
//
//   - The "wind_thresh" entry is an array of thresholds used to filter wind
//     speed values when computing VL1L2 vector partial sums.  Only those U/V
//     pairs that meet this wind speed criteria will be included in the sums.
//     Setting this threshold to NA will result in all U/V pairs being used.
//
//   - The "wind_logic" entry may be set to UNION, INTERSECTION, or SYMDIFF
//     and controls the logic for how the forecast and observed wind_thresh
//     settings are combined when filtering matched pairs of forecast and
//     observed wind speeds.
//
//   - The "init_time" entry specifies the initialization time in YYYYMMDD[_HH[MMSS]]
//     format.  This entry can be included in the "fcst" entry as shown below or
//     included in the "field" entry if the user would like to use different
//     initialization times for different fields.
//
//   - The "valid_time" entry specifies the valid time in YYYYMMDD[_HH[MMSS]]
//     format.  This entry can be included in the "fcst" entry as shown below or
//     included in the "field" entry if the user would like to use different
//     valid times for different fields.
//
//   - The "lead_time" entry specifies the lead time in HH[MMSS]
//     format.  This entry can be included in the "fcst" entry as shown below or
//     included in the "field" entry if the user would like to use different
//     lead times for different fields.
//
// It is only necessary to use the "init_time", "valid_time", and/or "lead_time"
// settings when verifying a file containing data for multiple output times.
// For example, to verify a GRIB file containing data for many lead times, you
// could use "lead_time" to specify the record to be verified.
//
// File-format specific settings for the "field" entry:
//
//    - GRIB1 and GRIB2:
//       - For custom GRIB tables, see note about MET_GRIB_TABLES.
//       - The "name" entry specifies a GRIB code number or abbreviation.
//         - GRIB1 Product Definition Section:
//           http://www.nco.ncep.noaa.gov/pmb/docs/on388/table2.html
//         - GRIB2 Product Definition Section:
//           http://www.nco.ncep.noaa.gov/pmb/docs/grib2/grib2_doc.shtml
//       - The "level" entry specifies a level type and value:
//          - ANNN for accumulation interval NNN
//          - ZNNN for vertical level NNN
//          - ZNNN-NNN for a range of vertical levels
//          - PNNN for pressure level NNN in hPa
//          - PNNN-NNN for a range of pressure levels in hPa
//          - LNNN for a generic level type
//          - RNNN for a specific GRIB record number
//       - The "GRIB_lvl_typ" entry specifies the level type.
//       - The "GRIB_lvl_val1" and "GRIB_lvl_val2" entries specify the first
//         and second level values.
//       - The "GRIB_ens" entry is a string which specifies NCEP's usage of the
//         extended PDS for ensembles.  Set to "hi_res_ctl", "low_res_ctl", "+n",
//         or "-n", for the n-th ensemble member.
//       - The "GRIB1_ptv" entry specifies the GRIB1 parameter table version number.
//       - The "GRIB1_code" entry specifies the GRIB1 code (wgrib kpds5 value).
//       - The "GRIB1_center" entry specifies the originating center.
//       - The "GRIB1_subcenter" entry specifies the originating subcenter.
//       - The "GRIB2_disc" entry specifies the GRIB2 discipline code.
//       - The "GRIB2_parm_cat" entry specifies the parameter category code.
//       - The "GRIB2_parm" entry specifies the parameter code.
//       - The "GRIB2_pdt" entry specifies the product definition template (Table 4.0).
//       - The "GRIB2_process" entry specifies the generating process (Table 4.3).
//       - The "GRIB2_cntr" entry specifies the originating center.
//       - The "GRIB2_ens_type" entry specifies the ensemble type (Table 4.6).
//       - The "GRIB2_der_type" entry specifies the derived product type (Table 4.7).
//
//    - NetCDF (from MET tools, CF-compliant, p_interp, and wrf_interp):
//       - The "name" entry specifies the NetCDF variable name.
//       - The "level" entry specifies the dimensions to be used:
//          - (i,...,j,*,*) for a single field, where i,...,j specifies fixed
//            dimension values and *,* specifies the two dimensions for the
//            gridded field.
//       e.g.
//           field = [
//             {
//               name       = "QVAPOR";
//               level      = "(0,5,*,*)";
//             },
//             {
//               name       = "TMP_P850_ENS_MEAN";
//               level      = [ "(*,*)" ];
//             }
//
//           ];
//
//
fcst = {
   cnt_thresh   = [ NA ];
   cnt_logic    = UNION;
   wind_thresh  = [ NA ];
   wind_logic   = UNION;
   message_type = [ "ADPSFC" ];
   init_time    = "20120619_12";
   valid_time   = "20120620_00";
   lead_time    = "12";

   field = [
      {
        name       = "APCP";
        level      = [ "A03" ];
        cat_thresh = [ >0.0, >=5.0 ];
      }
   ];
}

//
// The "obs" entry specifies the same type of information as "fcst", but for
// the observation data.  It will often be set to the same things as "fcst",
// as shown in the example below.  However, when comparing forecast and
// observation files of different format types, this entry will need to be set
// in a non-trivial way.  The length of the "obs.field" array must match the
// length of the "fcst.field" array.
//     e.g.
//         obs = fcst;
//
//     or
//
//         fcst = {
//           cnt_thresh  = [ NA ];
//           cnt_logic   = UNION;
//           wind_thresh = [ NA ];
//           wind_logic  = UNION;
//
//           field = [
//              {
//                 name       = "PWAT";
//                 level      = [ "L0" ];
//                 cat_thresh = [ >2.5 ];
//              }
//            ];
//         }
//
//
//         obs = {
//           cnt_thresh  = [ NA ];
//           cnt_logic   = UNION;
//           wind_thresh = [ NA ];
//           wind_logic  = UNION;
//
//           field = [
//              {
//                 name       = "IWV";
//                 level      = [ "L0" ];
//                 cat_thresh = [ >25.0 ];
//              }
//            ];
//         }
//
//
//   - The "message_type" entry is an array of point observation message types
//     to be used.  This only applies to the tools that verify against point
//     observations.  This may be specified once at the top-level "obs"
//     dictionary or separately for each "field" array element.  In the example
//     shown above, this is specified in the "fcst" dictionary and copied to "obs".
//   - The "message_type" would be placed in the "field" array element if more
//     than one "message_type" entry is desired within the config file.
//     e.g.
//     fcst = {
//       cnt_thresh  = [ NA ];
//       cnt_logic   = UNION;
//       wind_thresh = [ NA ];
//       wind_logic  = UNION;
//
//       field = [
//          {
//            message_type = [ "ADPUPA" ];
//            sid_exc      = [];
//            name       = "TMP";
//            level      = [ "P250", "P500", "P700", "P850", "P1000" ];
//            cat_thresh = [ <=273.0 ];
//          },
//          {
//            message_type = [ "ADPSFC" ];
//            sid_exc      = [ "KDEN", "KDET" ];
//            name       = "TMP";
//            level      = [ "Z2" ];
//            cat_thresh = [ <=273.0 ];
//          }
//       ];
//     }
//
//    - The "sid_exc" entry is an array of station ID groups indicating which
//      station ID's should be excluded from the verification tasks.  Each element
//      is either the name of a single station ID or the full path to a station ID
//      group file name.  A station ID group file consists of a name for the group
//      followed by a list of station ID's.  All of the station ID's indicated will
//      be placed into one long list of station ID's to be excluded.
//    - As with "message_type" above, the "sid_exc" setting can be placed in the
//      in the "field" array element to control which station ID's are excluded
//      for each verification task.
//
obs = fcst;

//
// The "climo_mean" dictionary specifies climatology data to read by the
// Grid-Stat, Point-Stat, and Ensemble-Stat tools.  It consists of several
// entires defining the climatology file names and fields to be used.
//
//   - The "file_names" entry specifies one or more file names containing
//     the gridded climatology data to be used.
//
//   - The "field" entry is an array of dictionaries, specified the same
//     way as those in the "fcst" and "obs" dictionaries.  If the array has
//     length zero, not climatology data will be read and all climatology
//     statistics will be written as missing data.  Otherwise, the array
//     length must match the length of "field" in the "fcst" and "obs"
//     dictionaries.
//
//   - The "regrid" dictionary defines how the climatology data should be
//     regridded to the verification domain.
//
//   - The "time_interp_method" entry specifies how the climatology data should
//     be interpolated in time to the forecast valid time:
//      - NEAREST for data closest in time
//      - UW_MEAN for average of data before and after
//      - DW_MEAN for linear interpolation in time of data before and after
//
//   - The "match_day" entry may be set to TRUE or FALSE.  When searching
//     climatology data, only consider times where the month matches the
//     forecast valid month.  Set match_day to TRUE or FALSE to define whether
//     the climatology day must also match.
//      - match_day = FALSE for monthly climatology
//      - match_day = TRUE  for daily climatology
//
//   - The "time_step" entry specifies the spacing of climatology data in
//     seconds.  Set to 60*60*6 = 21600 for 6-hourly data or 60*60 = 3600
//     for hourly data.
//
climo_mean = {

   file_name = [];
   field     = [];

   regrid = {
      method     = NEAREST;
      width      = 1;
      vld_thresh = 0.5;
   }

   time_interp_method = DW_MEAN;
   match_day          = FALSE;
   time_step          = 21600;
}

//
// The "mask_missing_flag" entry specifies how missing data should be handled
// in the Wavelet-Stat and MODE tools:
//    - "NONE" to perform no masking of missing data
//    - "FCST" to mask the forecast field with missing observation data
//    - "OBS" to mask the observation field with missing forecast data
//    - "BOTH" to mask both fields with missing data from the other
//
mask_missing_flag = BOTH;

//
// The "obs_window" entry is a dictionary specifying a beginning ("beg"
// entry) and ending ("end" entry) time offset values in seconds.  It defines
// the time window over which observations are retained for scoring. These time
// offsets are defined relative to a reference time t, as [t+beg, t+end].
// In PB2NC, the reference time is the PrepBufr files center time.  In
// Point-Stat and Ensemble-Stat, the reference time is the forecast valid time.
//
obs_window = {
   beg = -5400;
   end =  5400;
}

//
// The "mask" entry is a dictionary that specifies the verification masking
// regions to be used when computing statistics.  Each mask defines a
// geographic extent, and any matched pairs falling inside that area will be
// used in the computation of statistics.  Masking regions may be specified
// in the following ways:
//
//    - The "grid" entry is an array of named grids.  It contains a
//      comma-separated list of pre-defined NCEP grids over which to perform
//      verification.  An empty list indicates that no masking grids should be
//      used.  The standard NCEP grids are named "GNNN" where NNN indicates the
//      three digit grid number.  Supplying a value of "FULL" indicates that the
//      verification should be performed over the entire grid on which the data
//      resides.
//      http://www.nco.ncep.noaa.gov/pmb/docs/on388/tableb.html
//      The "grid" entry can be the gridded data file defining grid.
//
//    - The "poly" entry contains a comma-separated list of files that define
//      verification masking regions. These masking regions may be specified in
//      two ways: as a lat/lon polygon or using a gridded data file such as the
//      NetCDF output of the Gen-Vx-Mask tool.
//
//       - An ASCII file containing a lat/lon polygon.
//         Latitude in degrees north and longitude in degrees east.
//         The first and last polygon points are connected.
//         e.g. "MET_BASE/poly/EAST.poly" which consists of n points:
//              "poly_name lat1 lon1 lat2 lon2... latn lonn"
//
//         Several masking polygons used by NCEP are predefined in the
//         installed share/met/poly directory. Creating a new polygon is as
//         simple as creating a text file with a name for the polygon followed
//         by the lat/lon points which define its boundary. Adding a new masking
//         polygon requires no code changes and no recompiling. Internally, the
//         lat/lon polygon points are converted into x/y values in the grid. The
//         lat/lon values for the observation points are also converted into x/y
//         grid coordinates. The computations performed to check whether the
//         observation point falls within the polygon defined is done in x/y
//         grid space.
//
//       - The NetCDF output of the gen_vx_mask tool.
//
//       - Any gridded data file that MET can read may be used to define a
//         verification masking region. Users must specify a description of the
//         field to be used from the input file and, optionally, may specify a
//         threshold to be applied to that field. Once this threshold is applied,
//         any grid point where the resulting field is 0, the mask is turned off.
//         Any grid point where it is non-zero, the mask is turned on.
//         e.g. "sample.grib {name = \"TMP\"; level = \"Z2\";} >273"
//
//    - The "sid" entry is an array of strings which define groups of
//      observation station ID's over which to compute statistics. Each entry
//      in the array is either a filename of a comma-separated list.
//       - For a filename, the strings are whitespace-separated.  The first
//         string is the mask "name" and the remaining strings are the station
//         ID's to be used.
//       - For a comma-separated list, optionally use a colon to specify a name.
//         For "MY_LIST:SID1,SID2", name = MY_LIST and values = SID1 and SID2.
//       - For a comma-separated list of length one with no name specified, the
//         mask "name" and value are both set to the single station ID string.
//         For "SID1", name = SID1 and value = SID1.
//       - For a comma-separated list of length greater than one with no name
//         specified, the name is set to MASK_SID and the values are the station
//         ID's to be used.
//         For "SID1,SID2", name = MASK_SID and values = SID1 and SID2.
//       - The "name" of the station ID mask is written to the VX_MASK column
//         of the MET output files.
//
mask = {
   grid    = [ "FULL" ];
   poly    = [ "MET_BASE/poly/LMV.poly",
               "MET_BASE/out/gen_vx_mask/CONUS_poly.nc",
               "MET_BASE/sample_fcst/2005080700/wrfprs_ruc13_12.tm00_G212 \
               {name = \"TMP\"; level = \"Z2\";} >273"
             ];
   sid     = [ "CONUS.stations" ];
}

//
// The "ci_alpha" entry is an array of floats specifying the values for alpha
// to be used when computing confidence intervals.  Values of alpha must be
// between 0 and 1.  The confidence interval computed is 1 minus the alpha value.
// Therefore, an alpha value of 0.05 corresponds to a 95% confidence interval.
//
ci_alpha = [ 0.05, 0.10 ];

//
// The "boot" entry defines the parameters to be used in calculation of
// bootstrap confidence intervals. The interval variable indicates what method
// should be used for computing bootstrap confidence intervals:
//
//    - The "interval" entry specifies the confidence interval method:
//       - "BCA" for the BCa (bias-corrected percentile) interval method is
//         highly accurate but computationally intensive.
//       - "PCTILE" uses the percentile method which is somewhat less accurate
//         but more efficient.
//
//    - The "rep_prop" entry specifies a proportion between 0 and 1 to define
//      the replicate sample size to be used when computing percentile
//      intervals.  The replicate sample size is set to boot_rep_prop * n,
//      where n is the number of raw data points.
//
//      When computing bootstrap confidence intervals over n sets of matched
//      pairs, the size of the subsample, m, may be chosen less than or equal to
//      the size of the sample, n. This variable defines the size of m as a
//      proportion relative to the size of n.  A value of 1 indicates that the
//      size of the subsample, m, should be equal to the size of the sample, n.
//
//    - The "n_rep" entry defines the number of subsamples that should be taken
//      when computing bootstrap confidence intervals. This variable should be
//      set large enough so that when confidence intervals are computed multiple
//      times for the same set of data, the intervals do not change much.
//      Setting this variable to zero disables the computation of bootstrap
//      confidence intervals, which may be necessary to run MET in realtime or
//      near-realtime over large domains since bootstrapping is computationally
//      expensive. Setting this variable to 1000 indicates that bootstrap
//      confidence interval should be computed over 1000 subsamples of the
//      matched pairs.
//
//    - The "rng" entry defines the random number generator to be used in the
//      computation of bootstrap confidence intervals. Subsamples are chosen at
//      random from the full set of matched pairs. The randomness is determined
//      by the random number generator specified. Users should refer to detailed
//      documentation of the GNU Scientific Library for a listing of the random
//      number generators available for use.
//      http://www.gnu.org/software/gsl/manual/html_node/Random-Number-Generator-Performance.html
//
//    - The "seed" entry may be set to a specific value to make the computation
//      of bootstrap confidence intervals fully repeatable. When left empty
//      the random number generator seed is chosen automatically which will lead
//      to slightly different bootstrap confidence intervals being computed each
//      time the data is run. Specifying a value here ensures that the bootstrap
//      confidence intervals will be reproducable over multiple runs on the same
//      computing platform.
//
boot = {
   interval = PCTILE;
   rep_prop = 1.0;
   n_rep    = 0;
   rng      = "mt19937";
   seed     = "";
}

//
// The "interp" entry is a dictionary that specifies what interpolation or
// smoothing (for the Grid-Stat tools) methods should be applied.
// This dictionary may include the following entries:
//
//    - The "field" entry specifies to which field(s) the interpolation method
//      should be applied.  This does not apply when doing point verification
//      with the Point-Stat or Ensemble-Stat tools:
//       - "FCST" to interpolate/smooth the forecast field.
//       - "OBS" to interpolate/smooth the observation field.
//       - "BOTH" to interpolate/smooth both the forecast and the observation.
//
//    - The "vld_thresh" entry specifies a number between 0 and 1. When
//      performing interpolation over some neighborhood of points the ratio of
//      the number of valid data points to the total number of points in the
//      neighborhood is computed. If that ratio is less than this threshold,
//      the matched pair is discarded. Setting this threshold to 1, which is the
//      default, requires that the entire neighborhood must contain valid data.
//      This variable will typically come into play only along the boundaries of
//      the verification region chosen.
//
//    - The "shape" entry may be set to SQUARE or CIRCLE to specify the shape
//      of the smoothing area. 
//
//    - The "type" entry is an array of dictionaries, each specifying an
//      interpolation method.  Interpolation is performed over a N by N box
//      centered on each point, where N is the width specified.  Each of these
//      dictionaries must include:
//
//      - The "width" entry is an integer which specifies the dimension of the
//        interpolation box. The neighborhood is simply a square containing the
//        observation point. The width value specifies the width of that square.
//        A width value of 1 is interpreted as the nearest neighbor model grid
//        point to the observation point. A width of 2 defines a 2 x 2 square of
//        grid points around the observation point (the 4 closest model grid
//        points), while a width of 3 defines a 3 x 3 square of grid points
//        around the observation point, and so on.  For grid-to-grid comparisons,
//        the width must be odd.
//
//      - The "method" entry specifies the interpolation procedure to be
//        applied to the points in the box:
//         - MIN         for the minimum value
//         - MAX         for the maximum value
//         - MEDIAN      for the median value
//         - UW_MEAN     for the unweighted average value
//         - DW_MEAN     for the distance-weighted average value (weight = distance^-2)
//         - LS_FIT      for a least-squares fit
//         - BILIN       for bilinear interpolation (width = 2)
//         - NEAREST     for the nearest grid point (width = 1)
//         - BEST        for the value closest to the observation
//         - UPPER_LEFT  for the upper left grid point (width = 1)
//         - UPPER_RIGHT for the upper right grid point (width = 1)
//         - LOWER_RIGHT for the lower right grid point (width = 1)
//         - LOWER_LEFT  for the lower left grid point (width = 1)
//
//         The BUDGET and FORCE regridding options are not valid for interpolating.
//         For grid-to-grid comparisons, the only valid options are MIN, MAX, MEDIAN, and UW_MEAN.
//
interp = {
   field      = BOTH;
   vld_thresh = 1.0;
   shape      = SQUARE;

   type = [
      {
         method = UW_MEAN;
         width  = 1;
      }
   ];
}

//
// The "nbrhd" entry is a dictionary that is very similar to the "interp"
// entry.  It specifies information for computing neighborhood statistics in
// Grid-Stat.  This dictionary may include the following entries:
//
//    - The "vld_thresh" entry is described above.
//
//    - The "width" entry is as described above, and must be odd.
//
//    - The "cov_thresh" entry is an array of thresholds to be used when
//      computing categorical statistics for the neighborhood fractional
//      coverage field.
//
//    - The "shape" entry defines the shape of the neighborhood.
//      Valid values are "SQUARE" or "CIRCLE"
//
nbrhd = {
   width      = [ 1 ];
   cov_thresh = [ >=0.5 ];
   vld_thresh = 1.0;
   shape      = SQUARE;
}

//
// The "hira" entry is a dictionary that is very similar to the "interp" and
// "nbrhd" entries.  It specifies information for applying the High Resolution
// Assessment (HiRA) verification logic in Point-Stat.  HiRA is analogous to
// neighborhood verification but for point observations.  The HiRA logic
// converts scalar forecast values into a fractional coverage value which
// Point-Stat evaluates as if it were a probability forecast.  For each
// observation value, the forecast fractional coverage is computed as the ratio
// of forecast values around the observation that meet the categorical forecast
// threshold criteria.  If applying HiRA, users should enable matched pair or
// probabilistic line types (MPR, PCT, PSTD, PJC, or PRC) in the output_flag
// dictionary.  The number of HiRA output lines is determined by the number of
// categorical forecast thresholds and HiRA neighborhood widths chosen.
// This dictionary may include the following entries:
//
//    -  The "flag" entry is a boolean which toggles "hira"
//       on (TRUE) and off (FALSE).
//
//    - The "width" entry specifies the neighborhood size.  Since HiRA applies
//      to point observations, the width may be even or odd.
//
//    - The "vld_thresh" entry is as described above.
//
//    - The "cov_thresh" entry is an array of probabilistic thresholds used to
//      populate the Nx2 probabilistic contingency table written to the PCT
//      output line and used for computing probabilistic statistics.
//
//    - The "shape" entry defines the shape of the neighborhood.
//      Valid values are "SQUARE" or "CIRCLE"
//
hira = {
   flag       = FALSE;
   width      = [ 2, 3, 4, 5 ];
   vld_thresh = 1.0;
   cov_thresh = [ ==0.25 ];
   shape      = SQUARE;
}

//
// The "output_flag" entry is a dictionary that specifies what verification
// methods should be applied to the input data.  Options exist for each
// output line type from the MET tools.  Each line type may be set to one of:
//    - "NONE" to skip the corresponding verification method
//    - "STAT" to write the verification output only to the ".stat" output file
//    - "BOTH" to write to the ".stat" output file as well the optional
//      "_type.txt" file, a more readable ASCII file sorted by line type.
//
output_flag = {
   fho    = BOTH;  // Forecast, Hit, Observation Rates
   ctc    = BOTH;  // Contingency Table Counts
   cts    = BOTH;  // Contingency Table Statistics
   mctc   = BOTH;  // Multi-category Contingency Table Counts
   mcts   = BOTH;  // Multi-category Contingency Table Statistics
   cnt    = BOTH;  // Continuous Statistics
   sl1l2  = BOTH;  // Scalar L1L2 Partial Sums
   sal1l2 = BOTH;  // Scalar Anomaly L1L2 Partial Sums when climatological data is supplied
   vl1l2  = BOTH;  // Vector L1L2 Partial Sums
   val1l2 = BOTH;  // Vector Anomaly L1L2 Partial Sums when climatological data is supplied
   pct    = BOTH;  // Contingency Table Counts for Probabilistic Forecasts
   pstd   = BOTH;  // Contingency Table Statistics for Probabilistic Forecasts with Dichotomous outcomes
   pjc    = BOTH;  // Joint and Conditional Factorization for Probabilistic Forecasts
   prc    = BOTH;  // Receiver Operating Characteristic for Probabilistic Forecasts
   mpr    = BOTH;  // Matched Pair Data
   nbrctc = BOTH;  // Neighborhood Contingency Table Counts
   nbrcts = BOTH;  // Neighborhood Contingency Table Statistics
   nbrcnt = BOTH;  // Neighborhood Continuous Statistics
   isc    = BOTH;  // Intensity-Scale
   rhist  = BOTH;  // Rank Histogram
   phist  = BOTH;  // Probability Integral Transform Histogram
   orank  = BOTH;  // Observation Rank
   ssvar  = BOTH;  // Spread Skill Variance
}

//
// The "nc_pairs_flag" can be set either to a boolean value or a dictionary
// in either Grid-Stat, Wavelet-Stat or MODE. The dictionary (with slightly
// different entries for the various tools ... see the default config files)
// has individual boolean settings turning on or off the writing out of the
// various fields in the netcdf output file for the tool.  Setting all
// dictionary entries to false means the netcdf file will not be generated.
//
// "nc_pairs_flag" can also be set to a boolean value.  In this case, a value
// of true means to just accept the default settings (which will turn on
// the output of all the different fields).  A value of false means no
// netcdf output will be generated.
//
nc_pairs_flag   = {
   latlon     = TRUE;
   raw        = TRUE;
   diff       = TRUE;
   climo      = TRUE;
   weight     = FALSE;
   nbrhd      = FALSE;
   apply_mask = TRUE;
}

//
// The "ps_plot_flag" entry is a boolean value for Wavelet-Stat and MODE
// indicating whether a PostScript plot should be generated summarizing
// the verification.
ps_plot_flag = TRUE;

//
// The "grid_weight_flag" specifies how grid weighting should be applied
// during the computation of continuous statistics and partial sums.  It is
// meant to account for grid box area distortion and is often applied to global
// Lat/Lon grids.  It is only applied for grid-to-grid verification in Grid-Stat
// and Ensemble-Stat and is not applied for grid-to-point verification.
// Three grid weighting options are currently supported:
//
//    - "NONE" to disable grid weighting using a constant weight (default).
//    - "COS_LAT" to define the weight as the cosine of the grid point latitude.
//      This an approximation for grid box area used by NCEP and WMO.
//    - "AREA" to define the weight as the true area of the grid box (km^2).
//
// The weights are ultimately computed as the weight at each grid point divided
// by the sum of the weights for the current masking region.
//
grid_weight_flag = NONE;

//
// The "rank_corr_flag" entry is a boolean to indicate whether Kendall's Tau
// and Spearman's Rank Correlation Coefficients (in the CNT line type) should
// be computed.  Computing them over large datasets is computationally
// intensive and slows down the runtime significantly.
//
rank_corr_flag = FALSE;

//
// The "duplicate_flag" entry specifies how to handle duplicate point
// observations in Point-Stat and Ensemble-Stat:
//
//    - "NONE" to use all point observations (legacy behavior)
//    - "UNIQUE" only use a single observation if two or more observations match.
//             Matching observations are determined if they contain identical
//             latitude, longitude, level, elevation, and time information.
//             They may contain different observation values or station IDs
//
// The reporting mechanism for this feature can be activated by specifying
// a verbosity level of three or higher.  The report will show information
// about where duplicates were detected and which observations were used
// in those cases.
//
duplicate_flag = NONE;

//
// The "obs_summary" entry specifies how to compute statistics on
// observations that appear at a single location (lat,lon,level,elev)
// in Point-Stat and Ensemble-Stat.  Eight techniques are
// currently supported:
//
//    - "NONE" to use all point observations (legacy behavior)
//    - "NEAREST" use only the observation that has the valid
//      time closest to the forecast valid time
//    - "MIN" use only the observation that has the lowest value
//    - "MAX" use only the observation that has the highest value
//    - "UW_MEAN" compute an unweighted mean of the observations
//    - "DW_MEAN" compute a weighted mean of the observations based
//      on the time of the observation
//    - "MEDIAN" use the median observation
//    - "PERC" use the Nth percentile observation where N = obs_perc_value
//
// The reporting mechanism for this feature can be activated by specifying
// a verbosity level of three or higher.  The report will show information
// about where duplicates were detected and which observations were used
// in those cases.
//
obs_summary = NONE;

//
// Percentile value to use when obs_summary = PERC
//
obs_perc_value = 50;

//
// The "obs_quality" entry specifies the quality flag values that are to be
// retained and used for verification.  An empty list signifies that all
// point observations should be used, regardless of their quality flag value.
// The quality flag values will vary depending on the original source of the
// observations.  The quality flag values to retain should be specified as
// an array of strings, even if the values themselves are numeric.
//
obs_quality = [ "1", "2", "3", "9" ];

//
// The "met_data_dir" entry specifies the location of the internal MET data
// sub-directory which contains data files used when generating plots.  It
// should be set to the installed share/met directory so the MET tools can
// locate the static data files they need at run time.
//
met_data_dir = "MET_BASE";

//
// The "fcst_raw_plot" entry is a dictionary used by Wavelet-Stat and MODE
// containing colortable plotting information for the plotting of the raw
// forecast field:
//
//    - The "color_table" entry specifies the location and name of the
//      colortable file to be used.
//
//    - The "plot_min" and "plot_max" entries specify the range of data values.
//      If they are both set to 0, the MET tools will automatically rescale
//      the colortable to the range of values present in the data.  If they
//      are not both set to 0, the MET tools will rescale the colortable using
//      their values.
//
fcst_raw_plot = {
   color_table = "MET_BASE/colortables/met_default.ctable";
   plot_min = 0.0;
   plot_max = 0.0;
}

//
// The "obs_raw_plot", "wvlt_plot", and "object_plot" entries are dictionaries
// similar to the "fcst_raw_plot" described above.
//

//
// The "tmp_dir" entry is a string specifying the location where temporary
// files should be written.
//
tmp_dir = "/tmp";

//
// The "output_prefix" entry specifies a string to be included in the output
// file name.  The MET statistics tools construct output file names that
// include the tool name and timing information.  You can use this setting
// to modify the output file name and avoid naming conflicts for multiple runs
// of the same tool.
//
output_prefix  = "";

//
// The "version" entry specifies the version number of the configuration file.
// The configuration file version number should match the version number of
// the MET code being run.  This value should generally not be modified.
//
version = "V6.0";

////////////////////////////////////////////////////////////////////////////////
//
// Settings specific to individual tools
//
////////////////////////////////////////////////////////////////////////////////

////////////////////////////////////////////////////////////////////////////////
//
// Ascii2NcConfig_default
//
// A configuration file may be passed to the ascii2nc tool with the "-config"
// command line argument.  If not specified, the default configuration file
// will be used.
//
////////////////////////////////////////////////////////////////////////////////

//
// This feature was implemented to allow additional processing of observations
// with high temporal resolution. The "flag" entry toggles the "time_summary"
// on (TRUE) and off (FALSE).  Obs may be summarized across the user specified
// time period defined by the "beg" and "end" entries.  The "step" entry defines
// the time between intervals in seconds.  The "width" entry specifies the width
// of the summary interval in seconds.
//
// e.g. beg = "00";
//      end = "235959";
//      step = 300;
//      width = 600;
//
// This example does a 10-minute time summary every 5 minutes throughout the
// day.  The first interval will be from 23:55:00 the previous day through
// 00:04:59 of the current day.  The second interval will be from 0:00:00
// through 00:09:59. And so on.
//
// The summaries will only be calculated for the specified GRIB codes.
// The supported summaries are "min" (minimum), "max" (maximum), "range",
// "mean", "stdev" (standard deviation), "median" and "p##" (percentile, with
// the desired percentile value specified in place of ##).
//
// The "vld_freq" and "vld_thresh" options may be used to require that a certain
// ratio of observations must be present and contain valid data within the time
// window in order for a summary value to be computed.  The "vld_freq" entry
// defines the expected observation frequency in seconds.  For example, when
// summarizing 1-minute data (vld_freq = 60) over a 30 minute time window,
// setting "vld_thresh = 0.5" requires that at least 15 of the 30 expected
// observations be present and valid for a summary value to be written.  The
// default "vld_thresh = 0.0" setting will skip over this logic.
//
time_summary = {
  flag = FALSE;
  beg = "000000";
  end = "235959";
  step = 300;
  width = 600;
  grib_code = [ 11, 204, 211 ];
  type = [ "min", "max", "range", "mean", "stdev", "median", "p80" ];
  vld_freq = 0;
  vld_thresh = 0.0;
}

//
// The "message_type_map" entry is an array of dictionaries, each containing
// a "key" string and "val" string.  This defines a mapping of input strings
// to output message types.  This mapping is currently only applied when
// converting input little_r report types to output message types.
//
message_type_map = [
   { key = "FM-12 SYNOP";  val = "ADPSFC"; },
   { key = "FM-13 SHIP";   val = "SFCSHP"; },
   { key = "FM-15 METAR";  val = "ADPSFC"; },
   { key = "FM-18 BUOY";   val = "SFCSHP"; },
   { key = "FM-281 QSCAT"; val = "ASCATW"; },
   { key = "FM-32 PILOT";  val = "ADPUPA"; },
   { key = "FM-35 TEMP";   val = "ADPUPA"; },
   { key = "FM-88 SATOB";  val = "SATWND"; },
   { key = "FM-97 ACARS";  val = "AIRCFT"; }
];

////////////////////////////////////////////////////////////////////////////////
//
// EnsembleStatConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// The "ens" entry is a dictionary that specifies the fields for which ensemble
// products should be generated.  This is very similar to the "fcst" and "obs"
// entries.  This dictionary may include the following entries:
//
//    - The "ens_thresh" entry specifies a proportion between 0 and 1 to define
//      the required ratio of valid input ensemble member files.  If the ratio
//      of valid input ensemble files to expected ones is too low, the tool
//      will error out.
//
//    - The "vld_thresh" entry specifies a proportion between 0 and 1 to
//      define the required ratio of valid data points.  When computing
//      ensemble products, if the ratio of valid data values is too low, the
//      ensemble product will be set to bad data for that point.
//
//    - The "field" entry is as described above.  However, in this case, the
//      cat_thresh entry is used for calculating probabilities of exceeding
//      the given threshold.  In the default shown below, the probability of
//      accumulated precipitation > 0.0 mm and > 5.0 mm will be calculated
//      from the member accumulated precipitation fields and stored as an
//      ensemble field.
//
ens = {
   ens_thresh = 1.0;
   vld_thresh = 1.0;

   field = [
      {
         name       = "APCP";
         level      = "A03";
         cat_thresh = [ >0.0, >=5.0 ];
      }
   ];
}

//
// The fcst and obs entries is as described above and used by Ensemble Stat to
// compute rank histograms, probability integral transform histograms, spread-skill
// variance, and a other statistics. In this application the cat_thresh is not
// needed but "ens_ssvar_bin_size" and "ens_phist_bin_size" may be.
// The "ens_ssvar_bin_size" entry sets the width of the variance bins.  Smaller values
// provide the user with more flexibility in how data are binned during analysis.
// The "ens_phist_bin_size" is set to a value between 0 and 1.  The number of bins
// for the probability integral transform histogram in the PHIST line type is
// defined as the ceiling of 1.0 / ens_phist_bin_size.
//
fcst = {
   message_type = [ "ADPUPA" ];
   ens_ssvar_bin_size = 1;
   ens_phist_bin_size = 0.05;

   field = [
      {
         name  = "APCP";
         level = [ "A03" ];
      }
   ];
}

//
// The "ensemble_flag" entry is a dictionary of boolean value indicating
// which ensemble products should be generated:
//    - "mean" for the simple ensemble mean
//    - "stdev" for the ensemble standard deviation
//    - "minus" for the mean minus one standard deviation
//    - "plus" for the mean plus one standard deviation
//    - "min" for the ensemble minimum
//    - "max" for the ensemble maximum
//    - "range" for the range of ensemble values
//    - "vld_count" for the number of valid ensemble members
//    - "frequency" for the ensemble relative frequency meeting a threshold
//    - "rank" to write the rank for the gridded observation field to separate
//      NetCDF output file.
//    - "weight" to write the grid weights specified in grid_weight_flag to the
//      rank NetCDF output file.
//
ensemble_flag = {
   mean      = TRUE;
   stdev     = TRUE;
   minus     = TRUE;
   plus      = TRUE;
   min       = TRUE;
   max       = TRUE;
   range     = TRUE;
   vld_count = TRUE;
   frequency = TRUE;
   rank      = TRUE;
   weight    = FALSE;
}

//
// Random number generator used for random assignment of ranks when they
// are tied.
// http://www.gnu.org/software/gsl/manual/html_node/Random-Number-Generator-Performance.html
//
rng = {
   type = "mt19937";
   seed = "";
}

////////////////////////////////////////////////////////////////////////////////
//
// MODEAnalysisConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// MODE line options are used to create filters that determine which MODE output lines
// lines are read in and processed.  The MODE line options are numerous.  They fall into
// seven categories: toggles, multiple set string options, multiple set integer options,
// integer max/min options, date/time max/min options, floating-point max/min options, and
// miscellaneous options.  In order to be applied, the options must be uncommented (i.e.
// remove  the "//" marks) before running. These options are described in subsequent sections.
//

//
// Toggles: The MODE line options described in this section are shown in pairs.  These
// toggles represent parameters that can have only one (or none) of two values.  Any
// of these toggles may be left unspecified.  However, if neither option for toggle is
// indicated, the analysis will produce results that combine data from both toggles.
// This may produce unintended results.
//

//
// This toggle indicates whether forecast or observed lines should be used for analysis.
//
fcst      = FALSE;
obs       = FALSE;

//
// This toggle indicates whether single object or object pair lines should be used.
//
single    = FALSE;
pair      = FALSE;

//
// This toggle indicates whether simple object or object cluster object lines should be used.
//
simple    = FALSE;
cluster   = FALSE;

//
// This toggle indicates whether matched or unmatched object lines should be used.
//
matched   = FALSE;
unmatched = FALSE;

//
// Multiple-set string options: The following options set various string attributes.  They
// can be set multiple times on the command line but must be separated by spaces.  Each of
// these options must be indicated as a string.  String values that include spaces may be
// used by enclosing the string in quotation marks.
//

//
// This options specifies which model to use
//
//model    = [];

//
// These two options specify thresholds for forecast and observations objects to be used in
// the analysis, respectively.
//
//fcst_thr = [];
//obs_thr  = [];

//
// These options indicate the names of variables to be used in the analysis for forecast
// and observed fields.
//
//fcst_var = [];
//obs_var = [];

//
// These options indicate vertical levels for forecast and observed fields to be used in
// the analysis.
//
//fcst_lev = [];
//obs_lev = [];

//
// Multiple-set integer options: The following options set various integer attributes.
// Each of the following options may only be indicated as an integer.
//

//
// These options are integers of the form HH[MMSS] specifying an (hour-minute-second)
// lead time.
//
//fcst_lead       = [];
//obs_lead       = [];

//
// These options are integers of the form HH[MMSS] specifying an (hour-minute-second)
// valid hour.
//
//fcst_valid_hour = [];
//obs_valid_hour = [];

//
// These options are integers of the form HH[MMSS] specifying an (hour-minute-second)
// model initialization hour.
//
//fcst_init_hour  = [];
//obs_init_hour  = [];

//
// These options are integers of the form HHMMSS specifying an (hour-minute-second)
// accumulation time.
//
//fcst_accum      = [];
//obs_accum      = [];

//
// These options indicate the convolution radius used for forecast of observed objects,
// respectively.
//
//fcst_rad        = [];
//obs_rad        = [];

//
// Integer max/min options: These options set limits on various integer attributes.
// Leaving a maximum value unset means no upper limit is imposed on the value of
// the attribute.  The option works similarly for minimum values.
//

//
// These options are used to indicate minimum/maximum values for the area attribute
// to be used in the analysis.
//
//area_min              = 0;
//area_max              = 0;

//
// These options are used to indicate minimum/maximum values accepted for the area filter.
// The area filter refers to the number of non-zero values of the raw data found within
// the object.
//
//area_filter_min       = 0;
//area_filter_max       = 0;

//
// These options refer to the minimum/maximum values accepted for the intersection area
// attribute.
//
//intersection_area_min = 0;
//intersection_area_max = 0;

//
// These options refer to the minimum/maximum union area values accepted for analysis.
//
//union_area_min        = 0;
//union_area_max        = 0;

//
// These options refer to the minimum/maximum values for symmetric difference for objects
// to be used in the analysis.
//
//symmetric_diff_min    = 0;
//symmetric_diff_max    = 0;

//
// Date/time max/min options: These options set limits on various date/time attributes.
// The values can be specified in one of three ways:  First, the options may be indicated
// by a string of the form YYYMMDD_HHMMSS.  This specifies a complete calendar date and
// time.  Second, they may be indicated by a string of the form YYYYMMMDD_HH.  Here, the
// minutes and seconds are assumed to be zero.  The third way of indicating date/time
// attributes is by a string of the form YYYMMDD.  Here, hours, minutes, and seconds are
// assumed to be zero.
//

//
// These options indicate minimum/maximum values for the forecast valid time.
//
//fcst_valid_min = "";
//fcst_valid_max = "";

//
// These options indicate minimum/maximum values for the observation valid time.
//
//obs_valid_min  = "";
//obs_valid_max  = "";

//
// These options indicate minimum/maximum values for the forecast initialization time.
//
//fcst_init_min  = "";
//fcst_init_max  = "";

//
// These options indicate minimum/maximum values for the observation initialization time.
//
//obs_init_min   = "";
//obs_init_max   = "";

//
// Floating-point max/min options: Setting limits on various floating-point attributes.
// One may specify these as integers (i.e., without a decimal point), if desired.  The
// following pairs of options indicate minimum and maximum values for each MODE attribute
// that can be described as a floating-point number.  Please refer to "The MODE Tool"
// section on attributes in the MET User's Guide for a description of these attributes.
//

//centroid_x_min                 = 0.0;
//centroid_x_max                 = 0.0;

//centroid_y_min                 = 0.0;
//centroid_y_max                 = 0.0;

//centroid_lat_min               = 0.0;
//centroid_lat_max               = 0.0;

//centroid_lon_min               = 0.0;
//centroid_lon_max               = 0.0;

//axis_ang_min                   = 0.0;
//axis_ang_max                   = 0.0;

//length_min                     = 0.0;
//length_max                     = 0.0;

//width_min                      = 0.0;
//width_max                      = 0.0;

//aspect_ratio_min               = 0.0;
//aspect_ratio_max               = 0.0;

//curvature_min                  = 0.0;
//curvature_max                  = 0.0;

//curvature_x_min                = 0.0;
//curvature_x_max                = 0.0;

//curvature_y_min                = 0.0;
//curvature_y_max                = 0.0;

//complexity_min                 = 0.0;
//complexity_max                 = 0.0;

//intensity_10_min               = 0.0;
//intensity_10_max               = 0.0;

//intensity_25_min               = 0.0;
//intensity_25_max               = 0.0;

//intensity_50_min               = 0.0;
//intensity_50_max               = 0.0;

//intensity_75_min               = 0.0;
//intensity_75_max               = 0.0;

//intensity_90_min               = 0.0;
//intensity_90_max               = 0.0;

//intensity_user_min             = 0.0;
//intensity_user_max             = 0.0;

//intensity_sum_min              = 0.0;
//intensity_sum_max              = 0.0;

//centroid_dist_min              = 0.0;
//centroid_dist_max              = 0.0;

//boundary_dist_min              = 0.0;
//boundary_dist_max              = 0.0;

//convex_hull_dist_min           = 0.0;
//convex_hull_dist_max           = 0.0;

//angle_diff_min                 = 0.0;
//angle_diff_max                 = 0.0;

//area_ratio_min                 = 0.0;
//area_ratio_max                 = 0.0;

//intersection_over_area_min     = 0.0;
//intersection_over_area_max     = 0.0;

//complexity_ratio_min           = 0.0;
//complexity_ratio_max           = 0.0;

//percentile_intensity_ratio_min = 0.0;
//percentile_intensity_ratio_max = 0.0;

//interest_min                   = 0.0;
//interest_max                   = 0.0;

////////////////////////////////////////////////////////////////////////////////
//
// MODEConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// The "quilt" entry is a boolean to indicate whether all permutations of
// convolution radii and thresholds should be run.  If set to false, the number
// of forecast and observation convolution radii and thresholds must all match.
// One configuration of MODE will be run for each group of settings in those
// lists.  If set to true, the number of forecast and observation convolution
// radii must match and the number of forecast and observation convolution
// thresholds must match.  For N radii and M thresholds, NxM configurations of
// MODE will be run.
//
quilt = false;

//
// The object definition settings for MODE are contained within the "fcst" and
// "obs" entries:
//
//    - The "raw_thresh" entry specifies a threshold to be applied to the raw
//      data fields.  Any values not meeting the threshold are set to bad data.
//
//    - The "conv_radius" entry specifies the convolution radius in grid
//      squares.  The larger the convolution radius, the smoother the objects.
//      Multiple convolution radii may be specified as an array:
//        conv_radius = [ 5, 10, 15 ];
//
//    - The "conv_thresh" entry specifies the convolution threshold used to
//      define MODE objects.  The lower the threshold, the larger the objects.
//      Multiple convolution thresholds may be specified as an array:
//         conv_thresh = [ >=5.0, >=10.0, >=15.0 ];
//
//    - The "vld_thresh" entry is described above.
//
//    - The "area_thresh" entry specifies a threshold in grid squares for the
//      area of MODE objects.  Any objects not meeting this threshold are
//      discarded.
//
//    - The "inten_perc_value" entry specifies the intensity percentile value
//      of interest between 0 and 100.  The percentile set by this entry will
//      be output in addition to the standard intensity percentiles.
//
//    - The "inten_perc_thresh" entry specifies a threshold for the percentile
//      intensity of each MODE object.  Any objects not meeting this threshold
//      are discarded.
//
//    - The "merge_thresh" entry specifies a lower convolution threshold used
//      when the double-threshold merging method is applied.  The number of
//      merge thresholds must match the number of convolution thresholds.
//      Multiple merge thresholds may be specified as an array:
//         merge_thresh = [ >=1.0, >=2.0, >=3.0 ];
//
//    - The "merge_flag" entry specifies the merging methods to be applied:
//       - "NONE" for no merging
//       - "THRESH" for the double-threshold merging method.  Merge objects
//         that would be part of the same object at the lower threshold.
//       - "ENGINE" for the fuzzy logic approach comparing the field to itself
//       - "BOTH" for both the double-threshold and engine merging methods
//
fcst = {
   field = {
      name  = "APCP";
      level = "A03";
   }

   raw_thresh        = >=0.0;
   conv_radius       = 60.0/grid_res; // in grid squares
   conv_thresh       = >=5.0;
   vld_thresh        = 0.5;
   area_thresh       = >=0.0;
   inten_perc_value  = 100;
   inten_perc_thresh = >=0.0;
   merge_thresh      = >=1.25;
   merge_flag        = THRESH;
}

//
// The "grid_res" entry is the nominal spacing for each grid square in kilometers.
// The variable is not used directly in the code, but subsequent variables in the
// configuration files are defined in terms of it.  Therefore, setting the appropriately
// will help ensure that appropriate default values are used for these variables.
//
grid_res = 4;

//
// The "match_flag" entry specifies the matching method to be applied:
//    - "NONE" for no matching between forecast and observation objects
//    - "MERGE_BOTH" for matching allowing additional merging in both fields.
//      If two objects in one field match the same object in the other field,
//      those two objects are merged.
//    - "MERGE_FCST" for matching allowing only additional forecast merging
//    - "NO_MERGE" for matching with no additional merging in either field
//
match_flag = MERGE_BOTH;

//
// The "max_centroid_dist" entry specifies the maximum allowable distance in
// grid squares between the centroids of objects for them to be compared.
// Setting this to a reasonable value speeds up the runtime enabling MODE to
// skip unreasonable object comparisons.
//
max_centroid_dist = 800.0/grid_res;

//
// The weight variables control how much weight is assigned to each pairwise
// attribute when computing a total interest value for object pairs. The weights
// need not sum to any particular value but must be non-negative.  When the
// total interest value is computed, the weighted sum is normalized by the
// sum of the weights listed.
//
weight = {
   centroid_dist    = 2.0;
   boundary_dist    = 4.0;
   convex_hull_dist = 0.0;
   angle_diff       = 1.0;
   area_ratio       = 1.0;
   int_area_ratio   = 2.0;
   complexity_ratio = 0.0;
   inten_perc_ratio = 0.0;
   inten_perc_value = 50;
}

//
// The set of interest function variables listed define which values are of interest
// for each pairwise attribute measured.  The interest functions may be defined as
// a piecewise linear function or as an algebraic expression.  A piecewise linear
// function is defined by specifying the corner points of its graph.  An algebraic
// function may be defined in terms of several built-in mathematical functions.
//
interest_function = {

   centroid_dist = (
      (            0.0, 1.0 )
      (  60.0/grid_res, 1.0 )
      ( 600.0/grid_res, 0.0 )
   );

   boundary_dist = (
      (            0.0, 1.0 )
      ( 400.0/grid_res, 0.0 )
   );

   convex_hull_dist = (
      (            0.0, 1.0 )
      ( 400.0/grid_res, 0.0 )
   );

   angle_diff = (
      (  0.0, 1.0 )
      ( 30.0, 1.0 )
      ( 90.0, 0.0 )
   );

   corner   = 0.8;
   ratio_if = (
      (    0.0, 0.0 )
      ( corner, 1.0 )
      (    1.0, 1.0 )
   );

   area_ratio = ratio_if;

   int_area_ratio = (
      ( 0.00, 0.00 )
      ( 0.10, 0.50 )
      ( 0.25, 1.00 )
      ( 1.00, 1.00 )
   );

   complexity_ratio = ratio_if;

   inten_perc_ratio = ratio_if;
}

//
// The total_interest_thresh variable should be set between 0 and 1.  This threshold
// is applied to the total interest values computed for each pair of objects and
// is used in determining matches.
//
total_interest_thresh = 0.7;

//
// The print_interest_thresh variable determines which pairs of object attributes will
// be written to the output object attribute ASCII file.  The user may choose to set
// the print_interest_thresh to the same value as the total_interest_thresh, meaning
// that only object pairs that actually match are written to the output file. When set
// to zero, all object pair attributes will be written as long as the distance between
// the object centroids is less than the max_centroid_dist variable.
//
print_interest_thresh = 0.0;

//
// The MODE tool is not able to define objects that touch the edge of the grid.  After
// the convolution step is performed the outer columns and rows of data are zeroed out
// to enable MODE to identify objects.  The zero_border_size variable specifies how
// many outer columns and rows of data are to be zeroed out.
//
zero_border_size = 1;

//
// When applied, the plot_valid_flag variable indicates that only the region containing
// valid data after masking is applied should be plotted. TRUE indicates the entire domain
// should be plotted; FALSE indicates only the region containing valid data after masking
// should be plotted.
//
plot_valid_flag = FALSE;

//
// When applied, the plot_gcarc_flag variable indicates that the edges of polylines should
// be plotted using great circle arcs as opposed to straight lines in the grid.
//
plot_gcarc_flag = FALSE;

//
// The ct_stats_flag can be set to TRUE or FALSE to produce additional output, in the form
// of contingency table counts and statistics.
//
ct_stats_flag = TRUE;

//
// When MODE is run on global grids, this parameter specifies how many grid squares to
// shift the grid to the right.  MODE does not currently connect objects from one side
// of a global grid to the other, potentially causing objects straddling that longitude
// to be cut in half.  Shifting the grid by some amount enables the user to control
// where that longitude cut line occurs.
//
shift_right = 0;

////////////////////////////////////////////////////////////////////////////////
//
// PB2NCConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// The PB2NC tool filters out observations from PrepBufr files using the
// following criteria:
//  (1) by message type: supply a list of PrepBufr message types to retain
//  (2) by station id: supply a list of observation stations to retain
//  (3) by valid time: supply the beginning and ending time offset values
//      in the obs_window entry described above.
//  (4) by location: use the "mask" entry described below to supply either
//      an NCEP masking grid, a masking lat/lon polygon or a file to a
//      mask lat/lon polygon
//  (5) by elevation: supply min/max elevation values
//  (6) by report type: supply a list of report types to retain by using
//      pb_report_type and in_report_type entries described below
//  (7) by instrument type: supply a list of instrument type to
//      retain
//  (8) by vertical level: supply beg/end vertical levels by using the
//      level_range entry described below
//  (9) by variable type: supply a list of observation variable types to
//      retain by using the obs_grib_code entry described below
// (11) by quality mark: supply a quality mark threshold
// (12) Flag to retain values for all quality marks, or just the first
//      quality mark (highest): use the event_stack_flag described below
// (13) by data level category: supply a list of category types to
//      retain.
//
//      0 - Surface level (mass reports only)
//      1 - Mandatory level (upper-air profile reports)
//      2 - Significant temperature level (upper-air profile reports)
//      2 - Significant temperature and winds-by-pressure level
//          (future combined mass and wind upper-air reports)
//      3 - Winds-by-pressure level (upper-air profile reports)
//      4 - Winds-by-height level (upper-air profile reports)
//      5 - Tropopause level (upper-air profile reports)
//      6 - Reports on a single level
//          (e.g., aircraft, satellite-wind, surface wind,
//           precipitable water retrievals, etc.)
//      7 - Auxiliary levels generated via interpolation from spanning levels
//          (upper-air profile reports)
//

//
// The "message_type" entry is an array of message types to be retained.  An
// empty list indicates that all should be retained.
//
// List of valid message types:
//    ADPUPA AIRCAR AIRCFT ADPSFC ERS1DA GOESND GPSIPW
//    MSONET PROFLR QKSWND RASSDA SATEMP SATWND SFCBOG
//    SFCSHP SPSSMI SYNDAT VADWND
//    ANYAIR (= AIRCAR, AIRCFT)
//    ANYSFC (= ADPSFC, SFCSHP, ADPUPA, PROFLR)
//    ONLYSF (= ADPSFC, SFCSHP)
//
//    e.g. message_type[] = [ "ADPUPA", "AIRCAR" ];
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_1.htm
//
message_type = [];

//
// The "station_id" entry is an array of station ids to be retained or
// the filename which contains station ids. An array of station ids
// contains a comma-separated list. An empty list indicates that all
// stations should be retained.
//
// e.g. station_id = [ "KDEN" ];
//
station_id = [];

//
// The "elevation_range" entry is a dictionary which contains "beg" and "end"
// entries specifying the range of observing locations elevations to be
// retained.
//
elevation_range = {
   beg =  -1000;
   end = 100000;
}

//
// The "pb_report_type" entry is an array of PrepBufr report types to be
// retained.  The numeric "pb_report_type" entry allows for further stratification
// within message types.  An empty list indicates that all should be retained.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_4.htm
//
// e.g.
//   Report Type 120 is for message type ADPUPA but is only RAWINSONDE
//   Report Type 132 is for message type ADPUPA but is only FLIGHT-LEVEL RECON
//     and PROFILE DROPSONDE
//
pb_report_type  = [];

//
// The "in_report_type" entry is an array of input report type values to be
// retained.  The numeric "in_report_type" entry provides additional
// stratification of observations. An empty list indicates that all should
// be retained.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_6.htm
//
// e.g.
// Input Report Type 11 Fixed land RAOB and PIBAL by block and station number
// Input Report Type 12 Fixed land RAOB and PIBAL by call letters

in_report_type = [];

//
// The "instrument_type" entry is an array of instrument types to be retained.
// An empty list indicates that all should be retained.
//
instrument_type = [];

//
// The "level_range" entry is a dictionary which contains "beg" and "end"
// entries specifying the range of vertical levels (1 to 255) to be retained.
//
level_range = {
   beg = 1;
   end = 255;
}

//
// The "level_category" entry is an array of integers specifying which level
// categories should be retained:
//    0 = Surface level (mass reports only)
//    1 = Mandatory level (upper-air profile reports)
//    2 = Significant temperature level (upper-air profile reports)
//    2 = Significant temperature and winds-by-pressure level
//        (future combined mass and wind upper-air reports)
//    3 = Winds-by-pressure level (upper-air profile reports)
//    4 = Winds-by-height level (upper-air profile reports)
//    5 = Tropopause level (upper-air profile reports)
//    6 = Reports on a single level
//        (e.g., aircraft, satellite-wind, surface wind,
//         precipitable water retrievals, etc.)
//    7 = Auxiliary levels generated via interpolation from spanning levels
//        (upper-air profile reports)
// An empty list indicates that all should be retained.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_1.htm
//
level_category = [];

//
// The "obs_grib_code" entry is an array of  strings containing grib codes or
// corresponding grib code abbreviations to be retained or derived from the
// available observations.
//
// GRIB Codes to be RETAINED:
//    SPFH or 51 for Specific Humidity in kg/kg
//    TMP  or 11 for Temperature in K
//    HGT  or 7  for Height in meters
//    UGRD or 33 for the East-West component of the wind in m/s
//    VGRD or 34 for the North-South component of the wind in m/s
//
// GRIB Codes to be DERIVED:
//    DPT   or 17 for Dew point Temperature in K
//    WIND  or 32 for Wind Speed in m/s
//    RH    or 52 for Relative Humidity in %
//    MIXR  or 53 for Humidity Mixing Ratio in kg/kg
//    PRMSL or  2 for Pressure Reduced to Mean Sea Level in Pa
//
// http://www.nco.ncep.noaa.gov/pmb/docs/on388/table2.html
//
obs_grib_code = [ "SPFH", "TMP", "HGT", "UGRD", "VGRD" ];

//
// The "quality_mark_thresh" entry specifies the maximum quality mark value
// to be retained.  Observations with a quality mark LESS THAN OR EQUAL TO
// this threshold will be retained, while observations with a quality mark
// GREATER THAN this threshold will be discarded.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_7.htm
//
quality_mark_thresh = 2;

//
// The "event_stack_flag" entry is set to "TOP" or "BOTTOM" to
// specify whether observations should be drawn from the top of the event
// stack (most quality controlled) or the bottom of the event stack (most raw).
//
event_stack_flag = TOP;

////////////////////////////////////////////////////////////////////////////////
//
// SeriesAnalysisConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// Computation may be memory intensive, especially for large grids.
// The "block_size" entry sets the number of grid points to be processed
// concurrently (i.e. in one pass through a time series). Smaller values
// require less memory but increase the number of passes through the data.
//
block_size = 1024;

//
// Ratio of valid matched pairs to total length of series for a grid
// point.  If valid threshold is exceeded at that grid point the statistics
// are computed and stored.  If not, a bad data flag is stored. The default
// setting requires all data in the series to be valid.
//
//
vld_thresh = 1.0;

//
// Statistical output types need to be specified explicitly.  Refer to User's
// Guide for available output types.  To keep output file size reasonable,
// it is recommended to process a few output types at a time, especially if the
// grid is large.
//
output_stats = {
   fho    = [];
   ctc    = [];
   cts    = [];
   mctc   = [];
   mcts   = [];
   cnt    = [ "RMSE", "FBAR", "OBAR" ];
   sl1l2  = [];
   pct    = [];
   pstd   = [];
   pjc    = [];
   prc    = [];
}

////////////////////////////////////////////////////////////////////////////////
//
// STATAnalysisConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// The "jobs" entry is an array of STAT-Analysis jobs to be performed.
// Each element in the array contains the specifications for a single analysis
// job to be performed.  The format for an analysis job is as follows:
//
//    -job job_name
//    OPTIONAL ARGS
//
//    Where "job_name" is set to one of the following:
//
//       "filter"
//          To filter out the STAT or TCMPR lines matching the job filtering criteria
//          specified below and using the optional arguments below.  The
//          output STAT lines are written to the file specified using the
//          "-dump_row" argument.
//          Required Args: -dump_row
//
//       "summary"
//          To compute the mean, standard deviation, and percentiles
//          (0th, 10th, 25th, 50th, 75th, 90th, and 100th) for the statistic
//          specified using the "-line_type" and "-column" arguments.
//          For TCStat, the "-column" argument may be set to:
//             "TRACK" for track, along-track, and cross-track errors.
//             "WIND" for all wind radius errors.
//             "TI" for track and maximum wind intensity errors.
//             "AC" for along-track and cross-track errors.
//             "XY" for x-track and y-track errors.
//             "col" for a specific column name.
//             "col1-col2" for a difference of two columns.
//             "ABS(col or col1-col2)" for the absolute value.
//          Required Args: -line_type, -column
//          Optional Args (TCStat): -by column_name to specify case information
//                                  -out_alpha to override default alpha value of 0.05
//
//       "aggregate"
//          To aggregate the STAT data for the STAT line type specified using
//          the "-line_type" argument.  The output of the job will be in the
//          same format as the input line type specified.  The following line
//          types may be aggregated:
//          -line_type FHO, CTC, MCTC,
//                     SL1L2, SAL1L2, VL1L2, VAL1L2,
//                     PCT, NBRCNT, NBRCTC, ISC, RHIST, PHIST, SSVAR
//          Required Args: -line_type
//
//       "aggregate_stat"
//          To aggregate the STAT data for the STAT line type specified using
//          the "-line_type" argument.  The output of the job will be the line
//          type specified using the "-out_line_type" argument.  The valid
//          combinations of "-line_type" and "-out_line_type" are listed below.
//          -line_type FHO,   CTC,    -out_line_type CTS
//          -line_type MCTC           -out_line_type MCTS
//          -line_type SL1L2, SAL1L2, -out_line_type CNT
//          -line_type VL1L2, VAL1L2, -out_line_type WDIR (wind direction)
//          -line_type PCT,           -out_line_type PSTD, PJC, PRC
//          -line_type NBRCTC,        -out_line_type NBRCTS
//          -line_type ORANK,         -out_line_type RHIST, PHIST, SSVAR
//          -line_type MPR,           -out_line_type FHO, CTC, CTS,
//                                                   MCTC, MCTS, CNT,
//                                                   SL1L2, SAL1L2,
//                                                   PCT, PSTD, PJC, PRC,
//                                                   WDIR (wind direction)
//          Required Args:
//             -line_type, -out_line_type
//          Additional Required Args for -line_type MPR:
//             -out_thresh or -out_fcst_thresh and -out_obs_thresh
//             When -out_line_type FHO, CTC, CTS, MCTC, MCTS,
//                                 PCT, PSTD, PJC, PRC
//          Additional Optional Args for -line_type MPR:
//             -mask_grid, -mask_poly, -mask_sid
//             -out_thresh or -out_fcst_thresh and -out_obs_thresh
//             -out_cnt_logic
//             -out_wind_thresh or -out_fcst_wind_thresh and -out_obs_wind_thresh
//             -out_wind_logic
//             When -out_line_type WDIR
//          Additional Optional Args for -line_type ORANK -out_line_type PHIST, SSVAR:
//             -out_bin_size
//
//       "ss_index"
//          The skill score index job can be configured to compute a weighted
//          average of skill scores derived from a configurable set of
//          variables, levels, lead times, and statistics.  The skill score
//          index is computed using two models, a forecast model and a
//          reference model.  For each statistic in the index, a skill score
//          is computed as:
//             SS = 1 - (S[model]*S[model])/(S[reference]*S[reference])
//          Where S is the statistic.
//          Next, a weighted average is computed over all the skill scores.
//          Lastly, an index value is computed as:
//             Index = sqrt(1/(1-SS[avg]))
//          Where SS[avg] is the weighted average of skill scores.
//          Required Args:
//             Exactly 2 entries for -model, the forecast model and reference
//             For each term of the index:
//             -fcst_var, -fcst_lev, -fcst_lead, -line_type, -column, -weight
//             Where -line_type is CNT or CTS and -column is the statistic.
//             Optionally, specify other filters for each term, -fcst_thresh.
//
//       "go_index"
//          The GO Index is a special case of the skill score index consisting
//          of a predefined set of variables, levels, lead times, statistics,
//          and weights.
//          For lead times of 12, 24, 36, and 48 hours, it contains RMSE for:
//          - Wind Speed at the surface(b), 850(a), 400(a), 250(a) mb
//          - Dew point Temperature at the surface(b), 850(b), 700(b), 400(b) mB
//          - Temperature at the surface(b), 400(a) mB
//          - Height at 400(a) mB
//          - Sea Level Pressure(b)
//          Where (a) means weights of 4, 3, 2, 1 for the lead times, and
//          (b) means weights of 8, 6, 4, 2 for the lead times.
//
//          Required Args: None
//
//       "ramp"
//          The ramp job operates on a time-series of forecast and observed values
//          and is analogous to the RIRW (Rapid Intensification and Weakening) job
//          supported by the tc_stat tool.  The amount of change from one time to
//          the next is computed for forecast and observed values.  Those changes
//          are thresholded to define events which are used to populate a 2x2
//          contingency table.
//
//          Required Args:
//             -ramp_thresh (-ramp_thresh_fcst or -ramp_thresh_obs)
//                For DYDT, threshold for the amount of change required to define an event.
//                For SWING, threshold the slope.
//             -swing_width val
//                Required for the swinging door algorithm width.
//
//          Optional Args:
//             -ramp_type str
//                Overrides the default ramp definition algorithm to be used.
//                May be set to DYDT (default) or SWING for the swinging door algorithm.
//             -line_type str
//                Overrides the default input line type, MPR.
//             -out_line_type str
//                Overrides the default output line types of CTC and CTS.
//                Set to CTC,CTS,MPR for all possible output types.
//             -column fcst_column,obs_column
//                Overrides the default forecast and observation columns
//                to be used, FCST and OBS.
//             -ramp_time HH[MMSS] (-ramp_time_fcst or -ramp_time_obs)
//                Overrides the default ramp time interval, 1 hour.
//             -ramp_exact true/false (-ramp_exact_fcst or -ramp_exact_obs)
//                Defines ramps using an exact change (true, default) or maximum
//                change in the time window (false).
//             -ramp_window width in HH[MMSS] format
//             -ramp_window beg end in HH[MMSS] format
//                Defines a search time window when attempting to convert misses
//                to hits and false alarms to correct negatives.  Use 1 argument
//                to define a symmetric time window or 2 for an asymmetric window.
//                Default window is 0 0, requiring an exact match.
//
//    Job command FILTERING options to further refine the STAT data:
//       Each optional argument may be used in the job specification multiple
//       times unless otherwise indicated. When multiple optional arguments of
//       the same type are indicated, the analysis will be performed over their
//       union:
//
//       "-model            name"
//       "-fcst_lead        HHMMSS"
//       "-obs_lead         HHMMSS"
//       "-fcst_valid_beg   YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_valid_end   YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_valid_beg    YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_valid_end    YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_beg    YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_end    YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_init_beg     YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_init_end     YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_hour   HH[MMSS]"
//       "-obs_init_hour    HH[MMSS]"
//       "-fcst_valid_hour" HH[MMSS]
//       "-obs_valid_hour"  HH[MMSS]
//       "-fcst_var         name"
//       "-obs_var          name"
//       "-fcst_lev         name"
//       "-obs_lev          name"
//       "-obtype           name"
//       "-vx_mask          name"
//       "-interp_mthd      name"
//       "-interp_pnts      n"
//       "-fcst_thresh      t"
//       "-obs_thresh       t"
//       "-cov_thresh       t"
//       "-thresh_logic     UNION, or, ||
//                          INTERSECTION, and, &&
//                          SYMDIFF, symdiff, *
//       "-alpha            a"
//       "-line_type        type"
//       "-column           name"
//       "-weight           value"
//
//   Job command FILTERING options that may be used only when -line_type
//   has been listed once. These options take two arguments: the name of the
//   data column to be used and the min, max, or exact value for that column.
//   If multiple column eq/min/max/str options are listed, the job will be
//   performed on their intersection:
//
//       "-column_min    col_name value"     e.g. -column_min BASER 0.02
//       "-column_max    col_name value"
//       "-column_eq     col_name value"
//       "-column_thresh col_name threshold" e.g. -column_thresh FCST '>273&&<283'
//       "-column_str    col_name string" separate multiple filtering strings
//                                        with commas
//
//   Job command options to DEFINE the analysis job.  Unless otherwise noted,
//   these options may only be used ONCE per analysis job:
//
//       "-dump_row        path"
//
//       "-mask_grid       name"
//       "-mask_poly       file"
//       "-mask_sid        file|list" see description of "sid" entry above
//
//       "-out_line_type   name"
//       "-out_thresh      value" sets both -out_fcst_thresh and -out_obs_thresh
//       "-out_fcst_thresh value" multiple for multi-category contingency tables
//                                and probabilistic forecasts
//       "-out_obs_thresh  value" multiple for multi-category contingency tables
//       "-out_cnt_logic   value"
//
//       "-out_wind_thresh      value"
//       "-out_fcst_wind_thresh value"
//       "-out_obs_wind_thresh  value"
//       "-out_wind_logic       value"
//
//       "-out_bin_size    value"
//
//       "-out_alpha       value"
//
//       "-boot_interval   value"
//       "-boot_rep_prop   value"
//       "-n_boot_rep      value"
//       "-boot_rng        value"
//       "-boot_seed       value"
//
//       "-rank_corr_flag  value"
//       "-vif_flag        value"
//
//   For aggregate and aggregate_stat job types:
//
//       "-out_stat        path"   to write a .stat output file for the job including
//                                 the .stat header columns.  Multiple values for each
//                                 header column are written as a comma-separated list.
//       "-set_hdr col_name value" may be used multiple times to explicity specify what
//                                 should be written to the header columns of the output
//                                 .stat file.
//
jobs = [
   "-job filter         -line_type SL1L2 -vx_mask DTC165 -dump_row job_filter_SL1L2.stat",
   "-job summary        -line_type CNT   -alpha 0.050 -fcst_var TMP -dump_row job_summary_ME.stat -column ME",
   "-job aggregate      -line_type SL1L2 -vx_mask DTC165 -vx_mask DTC166 -fcst_var TMP -dump_row job_aggregate_SL1L2_dump.stat -out_stat job_aggregate_SL1L2_out.stat -set_hdr VX_MASK CONUS",
   "-job aggregate_stat -line_type SL1L2 -out_line_type CNT -vx_mask DTC165 -vx_mask DTC166 -fcst_var TMP -dump_row job_aggregate_stat_SL1L2_CNT_in.stat",
   "-job aggregate_stat -line_type MPR   -out_line_type CNT -vx_mask DTC165 -vx_mask DTC166 -fcat_var TMP -dump_row job_aggregate_stat_MPR_CNT_in.stat",
   "-job aggregate      -line_type CTC   -fcst_thresh <300.000 -vx_mask DTC165 -vx_mask DTC166 -fcst_var TMP -dump_row job_aggregate_CTC_in.stat",
   "-job aggregate_stat -line_type CTC   -out_line_type CTS -fcst_thresh <300.000 -vx_mask DTC165 -vx_mask DTC166 -fcst_var TMP -dump_row job_aggregate_stat_CTC_CTS_in.stat",
   "-job aggregate      -line_type MCTC  -column_eq N_CAT 4 -vx_mask DTC165 -vx_mask DTC166 -fcst_var APCP_24 -dump_row job_aggregate_MCTC_in.stat",
   "-job aggregate_stat -line_type MCTC  -out_line_type MCTS -column_eq N_CAT 4 -vx_mask DTC165 -vx_mask DTC166 -fcst_var APCP_24 -dump_row job_aggregate_stat_MCTC_MCTS_in.stat",
   "-job aggregate      -line_type PCT   -vx_mask DTC165 -vx_mask DTC166 -dump_row job_aggregate_PCT_in.stat",
   "-job aggregate_stat -line_type PCT   -out_line_type PSTD -vx_mask DTC165 -vx_mask DTC166 -dump_row job_aggregate_stat_PCT_PSTD_in.stat",
   "-job aggregate      -line_type ISC   -fcst_thresh >0.000 -vx_mask TILE_TOT -fcst_var APCP_12 -dump_row job_aggregate_ISC_in.stat",
   "-job aggregate      -line_type RHIST -obtype MC_PCP -vx_mask HUC4_1605 -vx_mask HUC4_1803 -dump_row job_aggregate_RHIST_in.stat",
   "-job aggregate      -line_type SSVAR -obtype MC_PCP -vx_mask HUC4_1605 -vx_mask HUC4_1803 -dump_row job_aggregate_SSVAR_in.stat",
   "-job aggregate_stat -line_type ORANK -out_line_type RHIST -obtype ADPSFC -vx_mask HUC4_1605 -vx_mask HUC4_1803 -dump_row job_aggregate_stat_ORANK_RHIST_in.stat"
];

//
// The "vif_flag" entry is a boolean to indicate whether a variance inflation
// factor should be computed when aggregating a time series of contingency
// table counts or partial sums.  The VIF is used to adjust the normal
// confidence intervals computed for the aggregated statistics.
//
vif_flag = FALSE;

////////////////////////////////////////////////////////////////////////////////
//
// WaveletStatConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// The "grid_decomp_flag" entry specifies how the grid should be decomposed in
// Wavelet-Stat into dyadic (2^n x 2^n) tiles:
//    - "AUTO" to tile the input data using tiles of dimension n by n where n
//      is the largest integer power of 2 less than the smallest dimension of
//      the input data.  Center as many tiles as possible with no overlap.
//    - "TILE" to use the tile definition specified below.
//    - "PAD" to pad the input data out to the nearest integer power of 2.
//
grid_decomp_flag = AUTO;

//
// The "tile" entry is a dictionary that specifies how tiles should be defined
// in Wavelet-Stat when the "grid_decomp_flag" is set to "TILE":
//
//    - The "width" entry specifies the dimension for all tiles and must be
//      an integer power of 2.
//
//    - The "location" entry is an array of dictionaries where each element
//      consists of an "x_ll" and "y_ll" entry specifying the lower-left (x,y)
//      coordinates of the tile.
//
tile = {
   width = 0;
   location = [
      {
         x_ll = 0;
         y_ll = 0;
      }
   ];
}

//
// The "wavelet" entry is a dictionary in Wavelet-Stat that specifies how the
// wavelet decomposition should be performed:
//
//    - The "type" entry specifies which wavelet should be used.
//
//    - The "member" entry specifies the wavelet shape.
//      http://www.gnu.org/software/gsl/manual/html_node/DWT-Initialization.html
//
//    - Valid combinations of the two are listed below:
//       - "HAAR" for Haar wavelet (member = 2)
//       - "HAAR_CNTR" for Centered-Haar wavelet (member = 2)
//       - "DAUB" for Daubechies wavelet (member = 4, 6, 8, 10, 12, 14, 16,
//         18, 20)
//       - "DAUB_CNTR" for Centered-Daubechies wavelet (member = 4, 6, 8, 10,
//          12, 14, 16, 18, 20)
//       - "BSPLINE" for Bspline wavelet (member = 103, 105, 202, 204, 206,
//         208, 301, 303, 305, 307, 309)
//       - "BSPLINE_CNTR" for Centered-Bspline wavelet (member = 103, 105, 202,
//          204, 206, 208, 301, 303, 305, 307, 309)
//
wavelet = {
   type   = HAAR;
   member = 2;
}

//
// The "obs_raw_plot", "wvlt_plot", and "object_plot" entries are dictionaries
// similar to the "fcst_raw_plot" described in the "Settings common to multiple
// tools" section.
//

////////////////////////////////////////////////////////////////////////////////
//
// WWMCARegridConfig_default
//
////////////////////////////////////////////////////////////////////////////////

//
// Specify the grid to which the data should be interpolated in one of the
// following ways:
//
//    - Name ("GNNN" where NNN indicates the three digit NCEP grid number)
//
//    - lambert Nx Ny lat_ll lon_ll lon_orient D_km R_km standard_parallel_1
//      [standard_parallel_2]
//
//    - stereo Nx Ny lat_ll lon_ll lon_orient D_km R_km lat_scale N|S
//
//    - latlon Nx Ny lat_ll lon_ll delta_lat delta_lon
//
//    - mercator Nx Ny lat_ll lon_ll lat_ur lon_ur
//
to_grid = "lambert 614 428 12.190 -133.459 -95.0 12.19058 6367.47 25.0";

//
// Supply the NetCDF output information
//
// e.g. variable_name = "Cloud_Pct";
//      units         = "percent";
//      long_name     = "cloud cover percent";
//      level         = "SFC";
//
variable_name = "";
units         = "";
long_name     = "";
level         = "";

//
// Maximum pixel age in minutes
//
max_minutes = 120;

//
// The WWMCA pixel age data is stored in binary data files in 4-byte blocks.
// The swap_endian option indicates whether the endian-ness of the data should
// be swapped after reading.
//
swap_endian = TRUE;

//
// By default, wwmca_regrid writes the cloud percent data specified on the
// command line to the output file.  This option writes the pixel age data,
// in minutes, to the output file instead.
//
write_pixel_age = FALSE;
