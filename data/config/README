////////////////////////////////////////////////////////////////////////////////
//
// Configuration file overview.
//
////////////////////////////////////////////////////////////////////////////////

The configuration files that control many of the MET tools contain formatted
ASCII text.  This format has been updated for METv4.0.

A configuration file entry is an entry name, followed by an equal sign =,
followed by an entry value, and is terminated by a semicolon ;.  The
configuration file itself is one large dictionary consisting of entries, some of
which are dictionaries themselves.

The configuration file language supports the following data types:
   - Dictionary:
      - Grouping of one or more entries enclosed by curly braces {}.
   - Array:
      - List of one or more entries enclosed by square braces [].
      - Array elements are separated by commas.
   - String:
      - A character string enclosed by double quotation marks "".
   - Integer:
      - A numeric integer value.
   - Float:
      - A numeric float value.
   - Boolean:
      - A boolean value (TRUE or FALSE).
   - Threshold:
      - A threshold type (<, <=, ==, !-, >=, or >) followed by a numeric value.
      - The threshold type may also be specified using two letter abbreviations
        (lt, le, eq, ne, ge, gt).
   - Piecewise-Linear Function (currently used only by MODE):
      - A list of (x, y) points enclosed in parenthesis ().
      - The (x, y) points are *NOT* separated by commas.

The context of a configuration entry matters.  If an entry cannot be found in
the expected dictionary, the MET tools recursively search for that entry in the
parent dictionaries, all the way up to the top-level configuration file
dictionary.  If you'd like to apply the same setting across all cases, you can
simply specify it once at the top-level.  Alternatively, you can specify a
setting at the appropriate dictionary level to have finer control over the
behavior.

In order to make the configuration files more readable, several descriptive
integer types have been defined in the ConfigConstants file.  These integer
names may be used on the right-hand side for many configuration file entries.

Each of the configurable MET tools expects a certain set of configuration
entries.  Examples of the MET configuration files can be found in data/config
and scripts/config.

When you pass a configuration file to a MET tool, the tool actually parses three
different configuration files in the following order:
   (1) Reads data/config/ConfigConstants to define constants.
   (2) Reads the default configuration file for the tool from data/config.
   (3) Reads the user-specified configuration file from the command line.

Many of the entries from step (2) are overwritten by the user-specified entries
from step (3).  Therefore, the configuration file you pass in on the command
line really only needs to contain entries that differ from the defaults.

The configuration file language supports the use of environment variables. When
scripting up many calls to the MET tools, you may find it convenient to use them.
They are specified as ${ENV_VAR}, where ENV_VAR is the name of the environment
variable.

The MET_BASE variable is defined in the code at compilation time as the path
to the top-level MET directory.  MET_BASE may be used in the MET configuration
files when specifying paths and the appropriate path will be substituted in.
If MET_BASE is defined as an enviroment variable, it's value will be used
instead of the one defined at compilation time.

An error in the syntax of a configuration file will result in an error from the
MET tool stating the location of the parsing error.

////////////////////////////////////////////////////////////////////////////////
//
// Configuration settings used by the MET tools.
//
////////////////////////////////////////////////////////////////////////////////

//
// The "model" entry specifies a name for the model being verified.  This name
// is written to the second column of the ASCII output generated.  If you're
// verifying multiple models, you should choose descriptive model names to
// distinguish between their output.
//
model = "WRF";

//
// The "fcst" entry is a dictionary containing information about the field(s)
// to be verified.  This dictionary may include the following entries:
//
//   - The "field" entry is an array of dictionaries, each specifying a
//     verification task.  Each of these dictionaries may include:
//
//      - The "name" entry specifies a name for the field.
//
//      - The "level" entry specifies level information for the field.
//
//      - Setting "name" and "level" is file-format specific.  See below.
//
//      - The "prob" entry is a boolean that specifies whether this field
//        contains probabilities.
//
//      - The "cat_thresh" entry is an array of thresholds to be used when
//        computing categorical statistics.
//
//   - The "file_type" entry specifies the input file type rather than letting
//     the code determine it itself.
//
//   - The "wind_thresh" entry is an array of thresholds used to filter wind
//     speed values when computing VL1L2 vector partial sums.  Only those U/V
//     pairs that meet this wind speed criteria will be included in the sums.
//     Setting this threshold to NA will result in all U/V pairs being used.
//
//   - The "message_type" entry is an array of point observation message types
//     to be used.  This may be specified once at the top-level "fcst"
//     dictionary or separately for each "field" array element.
//
// File-format specific settings for the "field" entry:
//
//    - GRIB1 and GRIB2:
//       - The "name" entry specifies a GRIB code number or abbreviation.
//       - The "level" entry specifies a level type and value:
//          - ANNN for accumulation interval NNN
//          - ZNNN for vertical level NNN
//          - ZNNN-NNN for a range of vertical levels
//          - PNNN for pressure level NNN in hPa
//          - PNNN-NNN for a range of pressure levels in hPa
//          - LNNN for a generic level type
//          - RNNN for a specific GRIB record number
//       - The "GRIB_lvl_typ" entry specifies the level type.
//       - The "GRIB_lvl_val1" and "GRIB_lvl_val2" entries specify the first
//         and second level values.
//       - The "GRIB1_ptv" entry specifies the GRIB1 parameter table version
//         number.
//       - The "GRIB1_rec" entry specifies the GRIB1 record number.
//       - The "GRIB2_disc" entry specifies the GRIB2 discipline code.
//       - The "GRIB2_parm_cat" entry specifies the parameter category code.
//       - The "GRIB2_parm" entry specifies the parameter code.
//
//    - NetCDF (from MET and Pinterp):
//       - The "name" entry specifies the NetCDF variable name.
//       - The "level" entry specifies the dimensions to be used:
//          - (i,...,j,*,*) for a single field, where i,...,j specifies fixed
//            dimension values and *,* specifies the two dimensions for the
//            gridded field.
//
fcst = {
   wind_thresh  = [ NA ];
   message_type = [ "ADPSFC" ];
   
   field = [
      {
        name       = "APCP";
        level      = [ "A03" ];
        cat_thresh = [ >0.0, >=5.0 ];
      }
   ];
};

//
// The "obs" entry specifies the same type of information as "fcst", but for
// the observation data.  It will often be set to the same things as "fcst",
// as shown in the example below.  However, when comparing forecast and
// observation files of different format types, this entry will need to be set
// in a non-trivial way.  The length of the "obs.field" array must match the
// length of the "fcst.field" array.
//
obs = fcst;

//
// The "ens" entry is a dictionary that specifies the fields for which ensemble
// products should be generated.  This is very similar to the "fcst" and "obs"
// entries.  This dictionary may include the following entries:
//
//    - The "ens_thresh" entry specifies a proportion between 0 and 1 to define
//      the required ratio of valid input ensemble member files.  If the ratio
//      of valid input ensemble files to expected ones is too lower, the tool
//      will error out.
//
//    - The "vld_thresh" entry specifies a proportion between 0 and 1 to
//      define the the required ratio of valid data points.  When computing
//      ensemble products, if the ratio of valid data values is too lower, the
//      ensemble product will be set to bad data for that point.
//
//    - The "field" entry is as described above.
//
ens = {
   ens_thresh = 1.0;
   vld_thresh = 1.0;

   field = [
      {
         name       = "APCP";
         level      = "A03";
         cat_thresh = [ >0.0, >=5.0 ];
      }
   ];
}

//
// The object definition settings for MODE are contained within the "fcst" and
// "obs" entries:
//
//    - The "raw_thresh" entry specifies a threshold to be applied to the raw
//      data fields.  Any values not meeting the threshold are set to bad data.
//
//    - The "conv_radius" entry specifies the convolution radius in grid
//      squares.  The larger the convolution radius, the smoother the objects.
//
//    - The "conv_thresh" entry specifies the convolution threshold used to
//      define MODE objects.  The lower the threshold, the larger the objects.
//
//    - The "vld_thresh" entry is described above.
//
//    - The "area_thresh" entry specifies a threshold in grid squares for the
//      area of MODE objects.  Any objects not meeting this threshold are
//      discarded.
//
//    - The "inten_perc_value" entry specifies the intensity percentile value
//      of interest between 0 and 100.
//
//    - The "inten_perc_thresh" entry specifies a threshold for the percentile
//      intensity of each MODE object.  Any objects not meeting this threshold
//      are discarded.
//
//    - The "merge_thresh" entry specifies a lower convolution threshold used
//      when the double-thresholding merging method is applied.
//
//    - The "merge_flag" entry specifies the merging methods to be applied:
//       - "NONE" for no merging
//       - "THRESH" for the double-threshold merging method.  Merge objects
//         that would be part of the same object at the lower threshold.
//       - "ENGINE" for the fuzzy logic approach comparing the field to itself
//       - "BOTH" for both the double-threshold and engine merging methods
//
fcst = {
   field = {
      name  = "APCP";
      level = "A03";
   };

   raw_thresh        = >=0.0;
   conv_radius       = 60.0/grid_res; // in grid squares
   conv_thresh       = >=5.0;
   vld_thresh        = 0.5;
   area_thresh       = >=0.0;
   inten_perc_value  = 100;
   inten_perc_thresh = >=0.0;
   merge_thresh      = >=1.25;
   merge_flag        = THRESH;
};

//
// The "match_flag" entry specifies the matching method to be applied:
//    - "NONE" for no matching between forecast and observation objects         = 1;
//    - "MERGE_BOTH" for matching allowing additional merging in both fields.
//      If two object in one field match the same object in the other field,
//      those two objects are merged.
//    - "MERGE_FCST" for matching allowing only additional forecast merging
//    - "NO_MERGE" for matching with no additional merging in either field
//
match_flag = MERGE_BOTH;

//
// The "max_centroid_dist" entry specifies the maximum allowable distance in
// grid squares between the centroids of objects for them to be compared.
// Setting this to a reasonable value speeds up the runtime enabling MODE to
// skip unreasonable object comparisons.
//
max_centroid_dist = 800.0/grid_res;

//
// The "mask_missing_flag" entry specifies how missing data should be handled
// in the Wavelet-Stat and MODE tools:
//    - "NONE" to perform no masking of missing data
//    - "FCST" to mask the forecast field with missing observation data
//    - "OBS" to mask the observation field with missing forecast data
//    - "BOTH" to mask both fields with missing data from the other
//
mask_missing_flag = BOTH;

//
// The "grid_decomp_flag" entry specifies how the grid should be decomposed in
// Wavelet-Stat into dyadic (2^n x 2^n) tiles:
//    - "AUTO" to tile the input data using tiles of dimension n by n where n
//      is the largest integer power of 2 less than the smallest dimension of
//      the input data.  Center as many tiles as possible with no overlap.
//    - "TILE" to use the tile definition specified below.
//    - "PAD" to pad the input data out to the nearest integer power of 2.
//
grid_decomp_flag = AUTO;

//
// The "tile" entry is a dictionary that specifies how tiles should be defined
// in Wavelet-Stat when the "grid_decomp_flag" is set to "TILE":
//
//    - The "width" entry specifies the dimension for all tiles and must be
//      an integer power of 2.
//
//    - The "location" entry is an array of dictionaries where each element
//      consists of an "x_ll" and "y_ll" entry specifying the lower-left (x,y)
//      coordinates of the tile.
//
tile = {
   width = 0;
   location = [
      {
         x_ll = 0;
         y_ll = 0;
      }
   ];
};

//
// The "wavelet" entry is a dictionary in Wavelet-Stat that specifies how the
// wavelet decomposition should be performed:
//
//    - The "type" entry specifies which wavelet should be used.
//
//    - The "member" entry specifies the wavelet shape.
//
//    - Valid combinations of the two are listed below:
//       - "HAAR" for Haar wavelet (member = 2)
//       - "HAAR_CNTR" for Centered-Haar wavelet
//       - "DAUB" for Daubechies wavelet (member = 4, 6, 8, 10, 12, 14, 16,
//         18, 20)
//       - "DAUB_CNTR" for Centered-Daubechies wavelet
//       - "BSPLINE" for Bspline wavelet (member = 103, 105, 202, 204, 206,
//         208, 301, 303, 305, 307, 309)
//       - "BSPLINE_CNTR" for Centered-Bspline wavelet
//
wavelet = {
   type   = HAAR;
   member = 2;
};

//
// The "obs_window" entry is a dictionary specifying an beginning ("beg"
// entry) and ending ("end" entry) time offset values in seconds.  These time
// offsets are defined relative to a reference time t, as [t+beg, t+end].
// In PB2NC, the reference time is the PrepBufr files center time.  In
// Point-Stat and Ensemble-Stat, the reference time is the forecast valid time.
//
obs_window = {
   beg = -5400;
   end =  5400;
}

//
// The PB2NC tool filters out observations from PrepBufr files using the
// following criteria:
//  (1) by message type: supply a list of PrepBufr message types to retain
//  (2) by station id: supply a list of observation stations to retain
//  (3) by valid time: supply starting and ending times in form
//      YYYY-MM-DD HH:MM:SS UTC
//  (4) by location: supply either an NCEP masking grid, a masking
//      lat/lon polygon or a file to a mask lat/lon polygon
//  (5) by elevation: supply min/max elevation values
//  (6) by report type (typ): supply a list of report types to retain
//  (7) by instrument type (itp): supply a list of instrument type to
//      retain
//  (8) by vertical level: supply min/max vertical levels
//  (9) by variable type: supply a list of variable types to retain
//      P, Q, T, Z, U, V
// (11) by quality mark: supply a quality mark threshold
// (12) Flag to retain values for all quality marks, or just the first
//      quality mark (highest)
// (13) by data level category: supply a list of category types to
//      retain.
//
//      0 - Surface level (mass reports only)
//      1 - Mandatory level (upper-air profile reports)
//      2 - Significant temperature level (upper-air profile reports)
//      2 - Significant temperature and winds-by-pressure level
//          (future combined mass and wind upper-air reports)
//      3 - Winds-by-pressure level (upper-air profile reports)
//      4 - Winds-by-height level (upper-air profile reports)
//      5 - Tropopause level (upper-air profile reports)
//      6 - Reports on a single level
//          (e.g., aircraft, satellite-wind, surface wind,
//           precipitable water retrievals, etc.)
//      7 - Auxiliary levels generated via interpolation from spanning levels
//          (upper-air profile reports)
//

//
// The "message_type" entry is an array of message types to be retianed.  An
// empty list indicates that all should be retained.
// List of valid message types:
//    ADPUPA AIRCAR AIRCFT ADPSFC ERS1DA GOESND GPSIPW
//    MSONET PROFLR QKSWND RASSDA SATEMP SATWND SFCBOG
//    SFCSHP SPSSMI SYNDAT VADWND
//    ANYAIR (= AIRCAR, AIRCFT)
//    ANYSFC (= ADPSFC, SFCSHP, ADPUPA, PROFLR)
//    ONLYSF (= ADPSFC, SFCSHP)
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_1.htm
//
message_type = [];

//
// The "station_id" entry is an array of station ids to be retained.  An empty
// list indicates that all stations should be retained.
//
station_id = [];

//
// The "elevation_range" entry is a dictionary which contains "beg" and "end"
// entries specifying the range of observing locations elevations to be
// retained.
//
elevation_range = {
   beg =  -1000;
   end = 100000;
}

//
// The "pb_report_type" entry is an array of PrepBufr report types to be
// retained.  An empty list indicates that all should be retained.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_4.htm
//
pb_report_type  = [];

//
// The "in_report_type" entry is an array of input report type values to be
// retained.  An empty list indicates that all should be retained.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_6.htm
//
in_report_type = [];

//
// The "instrument_type" entry is an array of instrument types to be retained.
// An empty list indicates that all should be retained.
//
instrument_type = [];

//
// The "level_range" entry is a dictionary which contains "beg" and "end"
// entries specifying the range of vertical levels (1 to 255) to be retained.
//
level_range = {
   beg = 1;
   end = 255;
}

//
// The "level_category" entry is an array of integers specifying which level
// categories should be retained:
//    0 = Surface level (mass reports only)
//    1 = Mandatory level (upper-air profile reports)
//    2 = Significant temperature level (upper-air profile reports)
//    2 = Significant temperature and winds-by-pressure level
//        (future combined mass and wind upper-air reports)
//    3 = Winds-by-pressure level (upper-air profile reports)
//    4 = Winds-by-height level (upper-air profile reports)
//    5 = Tropopause level (upper-air profile reports)
//    6 = Reports on a single level
//        (e.g., aircraft, satellite-wind, surface wind,
//         precipitable water retrievals, etc.)
//    7 = Auxiliary levels generated via interpolation from spanning levels
//        (upper-air profile reports)
// An empty list indicates that all should be retained.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_1.htm
//
level_category = [];

//
// The "obs_grib_code" entry is an array of  strings containing grib codes or
// corresponding grib code abbreviations to be retained or derived from the
// available observations.
//
// GRIB Codes to be RETAINED:
//    SPFH or 51 for Specific Humidity in kg/kg
//    TMP  or 11 for Temperature in K
//    HGT  or 7  for Height in meters
//    UGRD or 33 for the East-West component of the wind in m/s
//    VGRD or 34 for the North-South component of the wind in m/s
//
// GRIB Codes to be DERIVED:
//    DPT   or 17 for Dewpoint Temperature in K
//    WIND  or 32 for Wind Speed in m/s
//    RH    or 52 for Relative Humidity in %
//    MIXR  or 53 for Humidity Mixing Ratio in kg/kg
//    PRMSL or  2 for Pressure Reduced to Mean Sea Level in Pa
//
// http://www.nco.ncep.noaa.gov/pmb/docs/on388/table2.html
//
obs_grib_code = [ "SPFH", "TMP", "HGT", "UGRD", "VGRD" ];

//
// The "quality_mark_thresh" entry specifies the maximum quality mark value
// to be retained.  Observations with a quality mark LESS THAN OR EQUAL TO
// this threshold will be retained, while observations with a quality mark
// GREATER THAN this threshold will be discarded.
//
// http://www.emc.ncep.noaa.gov/mmb/data_processing/prepbufr.doc/table_7.htm
//
quality_mark_thresh = 2;

//
// The "event_stack_flag" entry specifies is set to "TOP" or "BOTTOM" to
// specify whether observations should be drawn from the top of the event
// stack (most quality controlled) or the bottom of the event stack (most raw).
//
event_stack_flag = TOP;

//
// The "mask" entry is a dictionary that specifies the verification masking
// regions to be used when computing statistics.  Each mask defines a
// geographic extent, and any matched pairs falling inside that area will be
// used in the computation of statistics.  Masking regions may be specified
// in the following ways:
//
//    - The "grid" entry is an array of named grids.  An empty list indicates
//      that no masking grids should be used.  The standard NCEP grids are
//      named "GNNN" where NNN indicates the three digit grid number.  Enter
//      "FULL" to score over the entire domain.
//      http://www.nco.ncep.noaa.gov/pmb/docs/on388/tableb.html
//
//    - The "poly" entry is an array of masking regions that may be specified
//      in the following ways:
//
//       - An ASCII file containing a lat/lon polygon.
//         Latitude in degrees north and longitude in degrees east.
//         By default, the first and last polygon points are connected.
//         e.g. "MET_BASE/data/poly/EAST.poly" which consists of n points:
//              "poly_name lat1 lon1 lat2 lon2... latn lonn"
//
//       - The NetCDF output of the gen_poly_mask tool.
//
//       - A gridded data file on the same grid as the forecast data followed
//         by a dictionary for the field to be used, and optionally, a
//         threshold to be applied.
//         e.g. "sample.grib {name = \"TMP\"; level = \"Z2\";} >273"
//
//       - The "sid" entry specifies an ASCII file containing a space-separated
//         list of station ID's to be used for point verification.  Each
//         station ID is treated as an individual masking region.
//
mask = {
   grid = [ "FULL" ];
   poly = [ "MET_BASE/data/poly/LMV.poly",
            "MET_BASE/out/gen_poly_mask/CONUS_poly.nc",
            "MET_BASE/data/sample_fcst/2005080700/wrfprs_ruc13_12.tm00_G212 \
            {name = \"TMP\"; level = \"Z2\";} >273"
          ];
   sid  = "CONUS.stations";
};

//
// The "ci_alpha" entry is an array of floats specifying the values for alpha
// to be used when computing confidence intervals.  Values of alpha must be
// between 0 and 1.
//
ci_alpha = [ 0.05, 0.10 ];

//
// The "boot" entry is a dictionary that specifies how bootstrap confidence
// intervals should be computed.  This dictionary may include the following
// entries:
//
//    - The "interval" entry specifies the confidence interval method:
//       - "BCA" for the BCa interval method is more accurate but slower.
//       - "PCTILE" uses the percentile method and is quicker.
//
//    - The "rep_prop" entry specifies a proportion between 0 and 1 to define
//      the replicate sample size to be used when computing percentile
//      intervals.  The replicate sample size is set to boot_rep_prop * n,
//      where n is the number of raw data points.
//
//    - The "n_rep" entry specifies the number of bootstrap replicates.  That
//      is the number of times each set of matched pair data should be
//      resampled when computing bootstrap confidence intervals.  A value of
//      zero disables the computation of bootstrap condifence intervals and
//      may speed up the runtime significantly.
//
//    - The "rng" entry specifies the name of the GSL random number generator.
//
//    - The "seed" entry specifies a seed value to be used when computing
//      bootstrap confidence intervals.  If left unspecified, the seed will
//      change for each run and the computed bootstrap confidence intervals
//      will not be reproducable.
//
boot = {
   interval = PCTILE;
   rep_prop = 1.0;
   n_rep    = 0;
   rng      = "mt19937";
   seed     = "";
};

//
// The "interp" entry is a dictionary that specifies what interpolation methods
// should be applied.  This dictionary may include the following entries:
//
//    - The "field" entry specifies to which field(s) the interpolation method
//      should be applied:
//       - "FCST" to interpolate/smooth the forecast field.
//       - "OBS" to interpolate/smooth the observation field.
//       - "BOTH" to interpolate/smooth both the forecast and the observation.
//
//    - The "vld_thresh" entry specifies a proportion between 0 and 1 to define
//      the required ratio of valid data points.  When interpolating, compute
//      a ratio of the number of valid data points to the total number of
//      points in the neighborhood.  If that ratio is less than this threshold,
//      skip the current point.
//
//    - The "type" entry is an array of dictionaries, each specifying an
//      interpolation method.  Interpolation is performed over a N by N box
//      centered on each point, where N is the width specified.  Each of these
//      dictionaries must include:
//
//      - The "width" entry specifies dimension of the interpolation box. For
//        grid-to-grid comparisons, the width must be odd.
//
//      - The "method" entry specifies the interpolation procedure to be
//        applied to the points in the box:
//           - "MIN" for the minimum value
//           - "MAX" for the maximum value
//           - "MEDIAN" for the median value
//           - "UW_MEAN" for the unweighted average value
//           - "DW_MEAN" for the distance-weighted average value
//           - "LS_FIT" for a least-squares fit
//           - "BILIN" for bilinear interpolation
//           - For grid-to-grid comparisons, only valid options are MIN, MAX,
//             MEDIAN, and UW_MEAN are options.
//
interp = {
   field      = BOTH;
   vld_thresh = 1.0;

   type = [
      {
         method = UW_MEAN;
         width  = 1;
      }
   ];
};

//
// The "nbrhd" entry is a dictionary that is very similar to the "interp"
// entry.  It specifies information for computing neighborhood statistics in
// Grid-Stat.  This dictionary may include the following entries:
//
//    - The "vld_thresh" entry is described above.
//
//    - The "width" entry is as descibed above, and must be odd.
//
//    - The "cov_thresh" entry is an array of thresholds to be used when
//      computing categorical statistics for the neighborhood fractional
//      coverage field.
//
nbrhd = {
   vld_thresh = 1.0;
   width      = [ 1 ];
   cov_thresh = [ >=0.5 ];
}

//
// The "output_flag" entry is a dictionary that specifies what verification
// methods should be applied to the input data.  Options exist for each
// output line type from the MET tools.  Each line type may be set to one of:
//    - "NONE" to skip the corresponding verification method
//    - "STAT" to write the verification output only to the ".stat" output file
//    - "BOTH" to write to the ".stat" output file as well the optional
//      "_type.txt" file, a more readable ASCII file sorted by line type.
//
output_flag = {
   fho    = BOTH;
   ctc    = BOTH;
   cts    = BOTH;
   mctc   = BOTH;
   mcts   = BOTH;
   cnt    = BOTH;
   sl1l2  = BOTH;
   vl1l2  = BOTH;
   pct    = BOTH;
   pstd   = BOTH;
   pjc    = BOTH;
   prc    = BOTH;
   nbrctc = BOTH;
   nbrcts = BOTH;
   nbrcnt = BOTH;
};

//
// The "ensemble_flag" entry is a dictionary of boolean value indicating
// which ensemble products should be generated:
//    - "mean" for the simple ensemble mean
//    - "stdev" for the ensemble standard deviation
//    - "minus" for the mean minus one standard deviation
//    - "plus" for the mean plus one standard deviation
//    - "min" for the ensemble minimum
//    - "max" for the ensemble maximum
//    - "range" for the range of ensemble values
//    - "vld_count" for the number of valid ensemble members
//    - "frequency" for the ensemble relative frequency meeting a threshold
//    - "rank" for the rank of the gridded observation field
//
ensemble_flag = {
   mean      = TRUE;
   stdev     = TRUE;
   minus     = TRUE;
   plus      = TRUE;
   min       = TRUE;
   max       = TRUE;
   range     = TRUE;
   vld_count = TRUE;
   frequency = TRUE;
   rank      = TRUE;
};

//
// The "nc_pairs_flag" entry is a boolean value for Grid-Stat indicating
// whether a NetCDF file containing the gridded forecast, observation, and
// difference fields for all verification tasks should be created.
//
nc_pairs_flag = TRUE;

//
// The "ps_plot_flag" entry is a boolean value for Wavelet-Stat and MODE
// indicating whether a PostScript plot should be generated summarizing
// the verification.
//
ps_plot_flag = TRUE;

//
// The "rank_corr_flag" entry is a boolean to indicate whether Kendall's Tau
// and Spearman's Rank Correlation Coefficients (in the CNT line type) should
// be computed.  Computing them over large datasets is computationally
// intensive and slows down the runtime significantly.
//
rank_corr_flag = FALSE;

//
// The "vif_flag" entry is a boolean to indicate whether a variance inflation
// factor should be computed when aggregating a time series of contingency
// table counts or partial sums.  The VIF is used to adjust the normal
// confidence intervals computed for the aggregated statistics.
//
vif_flag = FALSE;

//
// The "duplicate_flag" entry specifies how to handle duplicate point
// observations in Point-Stat and Ensemble-Stat.  Three techniques are
// currently supported:
//
//    - "NONE" to use all point observations (legacy behavior)
//    - "UNIQUE" if two or more observations match identically except in the
//       station ID field, only use a single observation
//    - "SINGLE" if two or more observations appear at a single location
//      (lat, lon, level, elv), use only the observation that has the valid
//      time closest to the forecast valid time
//
// The reporting mechanism for this feature can be activated by specifying
// a verbosity level of three or higher.  The report will show information
// about where duplicates were detected and which observations were used
// in those cases.
//
duplicate_flag = NONE;

//
// The "met_data_dir" entry specifies the location of the MET data
// sub-directory which contains data files used when generating plots.
//
met_data_dir = "MET_BASE/data";

//
// The "fcst_raw_plot" entry is a dictionary used by Wavelet-Stat and MODE
// containing colortable plotting information for the plotting of the raw
// forecast field:
//
//    - The "color_table" entry specifies the location of the colortable file.
//
//    - The "plot_min" and "plot_max" entries specify the range of data values.
//      If they are both set to 0, the MET tools will automatically rescale
//      the colortable to the range of values present in the data.  If they
//      are not both set to 0, the MET tools will rescale the colortable using
//      their values.
//
fcst_raw_plot = {
   color_table = "MET_BASE/data/colortables/met_default.ctable";
   plot_min = 0.0;
   plot_max = 0.0;
};

//
// The "obs_raw_plot", "wvlt_plot", and "object_plot" entries are dictionaies
// similar to the "fcst_raw_plot" described above.
//

//
// The "tmp_dir" entry is a string specifying the location where temporary
// files should be written.
//
tmp_dir = "/tmp";

//
// The "output_prefix" entry specifies a string to be included in the output
// file name.  The MET statistics tools construct output file names that
// include the tool name and timing information.  You can use this setting
// to modify the output file name and avoid naming conflicts for multiple runs
// of the same tool.
//
output_prefix  = "";

//
// The "version" entry specifies the version number of the configuration file.
// The configuration file version number should match the version number of
// the MET code being run.  This value should generally not be modified.
//
version = "V4.0";

//
// The "jobs" entry is an array of STAT-Analysis jobs to be performed.
// Each element in the array contains the specifications for a single analysis
// job to be performed.  The format for an analysis job is as follows:
//
//    -job job_name
//    OPTIONAL ARGS
//
//    Where "job_name" is set to one of the following:
//
//       "filter"
//          To filter out the STAT lines matching the criteria specified above
//          and using the optional arguments below.  The output STAT lines are
//          written to the file specified using the "-dump_row" option.
//          Required Args: -dump_row
//
//       "summary"
//          To compute the mean, standard deviation, and percentiles
//          (0th, 10th, 25th, 50th, 75th, 90th, and 100th) for the statistic
//          specified using the "-line_type" and "-column" arguments.
//          Required Args: -line_type, -column
//
//       "aggregate"
//          To aggregate the STAT data for the STAT line type specified using
//          the "-line_type" argument.  The output of the job will be in the
//          same format as the input line type specified.  The following line
//          types may be aggregated:
//          -line_type FHO, CTC, MCTC,
//                     SL1L2, SAL1L2, VL1L2, VAL1L2,
//                     PCT, NBRCTC, ISC, RHIST
//          Required Args: -line_type
//
//       "aggregate_stat"
//          To aggregate the STAT data for the STAT line type specified using
//          the "-line_type" argument.  The output of the job will be the line
//          type specified using the "-out_line_type" argument.  The valid
//          combinations of "-line_type" and "-out_line_type" are listed below.
//          -line_type FHO,   CTC,    -out_line_type CTS
//          -line_type MCTC           -out_line_type MCTS
//          -line_type SL1L2, SAL1L2, -out_line_type CNT
//          -line_type VL1L2, VAL1L2, -out_line_type WDIR (wind direction)
//          -line_type PCT,           -out_line_type PSTD, PJC, PRC
//          -line_type NBRCTC,        -out_line_type NBRCTS
//          -line_type MPR,           -out_line_type FHO, CTC, CTS,
//                                                   MCTC, MCTS, CNT,
//                                                   SL1L2, SAL1L2,
//                                                   PCT, PSTD, PJC, PRC
//          Required Args:
//             -line_type, -out_line_type
//          Additional Required Args for -line_type MPR:
//             -out_fcst_thresh, -out_obs_thresh
//             When -out_line_type FHO, CTC, CTS, MCTC, MCTS,
//                                 PCT, PSTD, PJC, PRC
//          Additional Optional Args for -line_type MPR:
//             -mask_grid, -mask_poly
//
//       "ss_index"
//          The skill score index job can be configured to compute a weighted
//          average of skill scores derived from a configurable set of
//          variables, levels, lead times, and statistics.  The skill score
//          index is computed using two models, a forecast model and a
//          reference model.  For each statistic in the index, a skill score
//          is computed as:
//             SS = 1 - (S[model]*S[model])/(S[reference]*S[reference])
//          Where S is the statistic.
//          Next, a weighted average is computed over all the skill scores.
//          Lastly, an index value is computed as:
//             Index = sqrt(1/(1-SS[avg]))
//          Where SS[avg] is the weighted average of skill scores.
//          Required Args:
//             Exactly 2 entries for -model, the forecast model and reference
//             For each term of the index:
//             -fcst_var, -fcst_lev, -fcst_lead, -line_type, -column, -weight
//             Where -line_type is CNT or CTS and -column is the statistic.
//             Optionally, specify other filters for each term, -fcst_thresh.
//
//       "go_index"
//          The GO Index is a special case of the Skill Score Index consisting
//          of a predefined set of variables, levels, lead times, statistics,
//          and weights.
//          For lead times of 12, 24, 36, and 48 hours, it contains RMSE for:
//          - Wind Speed at the surface(b), 850(a), 400(a), 250(a) mb
//          - Dewpoint Temperature at the surface(b), 850(b), 700(b), 400(b) mB
//          - Temperature at the surface(b), 400(a) mB
//          - Height at 400(a) mB
//          - Sea Level Pressure(b)
//          Where (a) means weights of 4, 3, 2, 1 for the lead times, and
//          (b) means weights of 8, 6, 4, 2 for the lead times.
//          Required Args: None
//
//    Job command FILTERING options to further refine the STAT data:
//       Each optional argument may be used in the job specification multiple
//       times unless otherwise indicated. When multiple optional arguments of
//       the same type are indicated, the analysis will be performed over their
//       union:
//
//       "-model          name"
//       "-fcst_lead      HHMMSS"
//       "-obs_lead       HHMMSS"
//       "-fcst_valid_beg YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_valid_end YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_valid_beg  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_valid_end  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_beg  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_end  YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_init_beg   YYYYMMDD[_HH[MMSS]]" (use once)
//       "-obs_init_end   YYYYMMDD[_HH[MMSS]]" (use once)
//       "-fcst_init_hour HH[MMSS]"
//       "-obs_init_hour  HH[MMSS]"
//       "-fcst_var       name"
//       "-obs_var        name"
//       "-fcst_lev       name"
//       "-obs_lev        name"
//       "-obtype         name"
//       "-vx_mask        name"
//       "-interp_mthd    name"
//       "-interp_pnts    n"
//       "-fcst_thresh    t"
//       "-obs_thresh     t"
//       "-cov_thresh     t"
//       "-alpha          a"
//       "-line_type      type"
//       "-column         name"
//       "-weight         value"
//
//   Job command FILTERING options used when the -line_type has been set to a
//   single value.  These options take two arguments, the name of the data
//   column to be used and the min, max, or exact value for that column.
//   If multiple column eq/min/max/str options are listed, the job will be
//   performed on their intersection:
//
//       "-column_min col_name value"
//       "-column_max col_name value"
//       "-column_eq  col_name value"
//       "-column_str col_name string" separate multiple filtering strings
//                                     with commas
//
//   Job command options to DEFINE the analysis job.  Unless otherwise noted,
//   these options may only be used ONCE per analysis job:
//
//       "-dump_row        path"
//
//       "-mask_grid       name"
//       "-mask_poly       file"
//
//       "-out_line_type   name"
//       "-out_fcst_thresh value" multiple for multi-category contingency tables
//                                and probabilistic forecasts
//       "-out_obs_thresh  value" multiple for multi-category contingency tables
//       "-out_alpha       value"
//
//       "-boot_interval   value"
//       "-boot_rep_prop   value"
//       "-n_boot_rep      value"
//       "-boot_rng        value"
//       "-boot_seed       value"
//
//       "-rank_corr_flag  value"
//       "-vif_flag        value"
//
jobs[] = [
   "-job filter -dump_row ./filter_job.stat"
];
